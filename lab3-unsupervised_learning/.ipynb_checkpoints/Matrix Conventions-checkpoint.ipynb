{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/vs265header.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> A Note on Matrix Conventions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there was confusion during Lab 2 about the course's convention on the orientation of matrices and vectors, I'm going to take minute to make them explicit and explain where they come from. Note that this convention is shared with the Hertz, Krogh, Palmer book, and so the equations that appear in Chapter 8 (p197 ff) can be directly translated into code for this lab.\n",
    "\n",
    "In the following, we'll be using the convention  that the weight vectors for each neuron are *the same shape as the data vectors*. Both the data and the weight vectors are columns in this problem set. This means that the columns of the weight matrix are the weight vectors for individual output neurons.\n",
    "\n",
    "With this convention in hand, what happens when we try to compute the ouput of a neuron given an input vector of length $N$? Note that if we try to multiply (the `@` symbol in numpy) two column vectors, we run into trouble, since the shapes don't line up: we'll be multiplying an $N$ by $1$ vector by another $N$ by $1$ vector, and $1$ is not equal to $N$!\n",
    "\n",
    "An easy solution suggests itself: transpose the first vector, so that we have $1$ by $N$ multiplying $N$ by $1$, and then we'll have a perfectly acceptable dot product on our hands. Even better, the output is $1$ by $1$, which makes sense since each neuron's output is just a single scalar value.\n",
    "\n",
    "The critical take-away here is this - **transpose the weights before computing the outputs**. This applies to weight vectors and weight matrices. Note that this convention has implications for how we compute the weight updates et cetera . \n",
    "\n",
    "Note also that is is critical that you maintain the row or column shape of vectors when you pull them out of matrices. Numpy doesn't naturally do this. Instead, it converts both row and column vectors into one-dimensional arrays. Check out the code cell below for a demonstration of the problem and the solution. First, we generate a random matrix, and then we pull a column vector out of it as a 1-D array, a column vector, and a row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We start off with the array: \n",
      "\n",
      "[[3 6]\n",
      " [2 1]\n",
      " [2 8]]\n",
      "\n",
      "A[:,1] gives us the array:\n",
      "\n",
      "[6 1 8]\n",
      "\n",
      "which has shape (3,)\n",
      "\n",
      "A[:,1,None] gives us the array: \n",
      "\n",
      "[[6]\n",
      " [1]\n",
      " [8]]\n",
      "\n",
      "which has shape (3, 1), meaning it's a column vector.\n",
      "\n",
      "A[None,:,1] gives us the array:\n",
      "\n",
      "[[6 1 8]]\n",
      "\n",
      "which has shape (1, 3), meaning it's a row vector.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.randint(1,10,size=(3,2))\n",
    "print(\"We start off with the array: \\n\\n\"+str(A) +\"\\n\")\n",
    "oneDimensionalSlice = A[:,1]\n",
    "print(\"A[:,1] gives us the array:\\n\\n\"+str(oneDimensionalSlice) +\"\\n\" )\n",
    "print(\"which has shape \" +str(oneDimensionalSlice.shape)+\"\\n\")\n",
    "\n",
    "columnVector = A[:,1,None]\n",
    "print(\"A[:,1,None] gives us the array: \\n\\n\"+str(columnVector)+\"\\n\" )\n",
    "print(\"which has shape \" +str(columnVector.shape)+\", meaning it's a column vector.\\n\")\n",
    "\n",
    "rowVector = A[None,:,1]\n",
    "print(\"A[None,:,1] gives us the array:\\n\\n\"+str(rowVector)+\"\\n\")\n",
    "print(\"which has shape \" +str(rowVector.shape)+\", meaning it's a row vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Aside\n",
    "\n",
    "If you're happy to simply adopt the above as a convention, read no further. If you're curious why this is a sensible convention, read on. It touches on some deeper linear algebra, so if you've never taken a full linear algebra course, don't worry if you don't totally follow the argument.\n",
    "\n",
    "Having the weight vectors and data vectors both be the same shape lets us think of the weight vectors as elements of the same vector space as the data. We'll be plotting the weight vectors and the data vectors together repeatedly in this course, and especially in this problem set, so this is a key move.\n",
    "\n",
    "But the weights and the data aren't really the same kind of mathematical object: in fact, the weight vectors represent *functions on the data vectors* rather than data vectors themselves. Each neuron computes its (scalar-valued) output as a linear combination of its inputs -- weight$_1 *$ input$_1$ + weight$_2 *$ input$_2 ...$ and so on -- and so the function computed by the neuron is what's called a *linear functional* -- a linear function that takes in vectors and spits out scalars.\n",
    "\n",
    "The set of all linear functionals on a vector space $\\mathcal{V}$ is itself a vector space, called the *dual space of * $\\mathcal{V}$, also denoted $\\mathcal{V}^*$. Under certain conditions, we can map vectors from $\\mathcal{V}$ to linear functionals from $\\mathcal{V}^*$, and we do so by *transposing* the vector.\n",
    "\n",
    "That's why we needed to transpose the weight vector before multiplying -- so that we could convert it into a linear functional on the data. Note that the same logic applies if we have a weight matrix -- each weight vector is a column of the matrix, and we transpose the matrix so that it becomes a row instead. When we multiply the matrix with the vector, each one of those rows produces a single scalar value, and the collection of those scalars is a vector -- a vector of neuron outputs.\n",
    "\n",
    "This distinction between vectors and linear functionals also explains why you can't dot-product two column vectors -- they are both vectors from $\\mathcal{V}$, and so they can't be functions on $\\mathcal{V}$. But transposing the first one turns it into a linear functional from $\\mathcal{V^*}$, and so we are free to compute the dot-product of the vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
