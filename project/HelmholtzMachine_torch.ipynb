{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "floatX = T.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create shared variables for using gpu\n",
    "def shared_dataset(data, borrow=True, data_types=['float32','int32']):\n",
    "    if type(data) is not list:\n",
    "        data = list(data)\n",
    "    output = []\n",
    "    for i, x in enumerate(data):\n",
    "        output.append(T.as_tensor(np.asarray(x, dtype=data_types[i]), borrow=borrow))\n",
    "    return output\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    # get path/file for dataset\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the current directory.\n",
    "        new_path = os.path.join(os.curdir, dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "    # download from website\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "    # load from pickle\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # set test/valid/train sets\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    # combine datasets\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzLayer(object):\n",
    "    '''\n",
    "    Helmholtz layer for Helmholtz Machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input: theano matrix, input to layer (data or output of previous layer)\n",
    "    n_in: int, number of input units\n",
    "    n_out: int, number of hidden units\n",
    "    unit: str, hidden unit type ['binary' (default) or 'gaussian']\n",
    "    top_layer: bool, True/False layer is top layer\n",
    "    k: float, fraction of most active units to keep; less active units are set to 0\n",
    "        [default: 0, no units set to 0]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzLayer\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Gaussian units assume 0 mean, 1 std (fixed sigma).\n",
    "    '''\n",
    "    def __init__(self, input, n_in, n_out, unit='binary', top_layer=False, k=T.tensor(0, dtype=T.int32)):\n",
    "        # init vars\n",
    "        self.input = input\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.unit = unit\n",
    "        self.top_layer = top_layer\n",
    "        self.k = k\n",
    "        \n",
    "        # recognition weights\n",
    "        self.WR = 0.01 * T.randn((n_in,n_out), dtype=floatX)\n",
    "        self.WR.requires_grad=True\n",
    "        # recognition biases\n",
    "        self.bR = T.zeros((n_out,), dtype=floatX, requires_grad=True)\n",
    "        # generative weights\n",
    "        self.WG = 0.01 * T.randn((n_out,n_in), dtype=floatX)\n",
    "        self.WG.requires_grad=True\n",
    "        # generative biases\n",
    "        self.bG = T.zeros((n_in,), dtype=floatX, requires_grad=True)\n",
    "        \n",
    "        # momentum\n",
    "        self.WR_inc = T.zeros((n_in,n_out), dtype=floatX)\n",
    "        self.bR_inc = T.zeros((n_out,), dtype=floatX)\n",
    "        self.WG_inc = T.zeros((n_out,n_in), dtype=floatX) \n",
    "        self.bG_inc = T.zeros((n_in,), dtype=floatX)\n",
    "        \n",
    "        # if top_layer, remove shared WR, bR, WG\n",
    "        if self.top_layer:\n",
    "            self.WR = T.zeros_like(self.WR)\n",
    "            self.bR = T.zeros_like(self.bR) \n",
    "            self.WG = T.zeros_like(self.WG) \n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.bG]\n",
    "            self.rec_params = []\n",
    "            self.inc_params = [self.bG_inc]\n",
    "        else:\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.WG, self.bG]\n",
    "            self.rec_params = [self.WR, self.bR]\n",
    "            self.inc_params = [self.WG_inc, self.bG_inc, self.WR_inc, self.bR_inc]\n",
    "        \n",
    "        # set output\n",
    "        self.set_output()\n",
    "        \n",
    "        # init reconstr, top_down\n",
    "        self.reconstr = None\n",
    "        self.top_down = None\n",
    "        \n",
    "    def activation(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = T.Tensor.sigmoid(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = u\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def sample(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = T.bernoulli(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = T.add(u, T.normal(u, T.ones_like(u)))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def prob(self, u, x, unit):\n",
    "        if unit == 'binary':\n",
    "            p = T.pow(u, x) * T.pow(1. - u, 1. - x)\n",
    "        elif unit == 'gaussian':\n",
    "            p = (1./T.sqrt(2. * np.pi)) * T.exp(-T.pow((x - u), 2.)/2.)\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return p\n",
    "    \n",
    "    def propup(self, v):\n",
    "        pre_act_h = T.mm(v, self.WR) + self.bR\n",
    "        # if k-sparse, apply\n",
    "        pre_act_h = self.k_sparse(pre_act_h, self.k)\n",
    "        return self.activation(pre_act_h, self.unit)\n",
    "    \n",
    "    def propdown(self, h):\n",
    "        pre_act_v = T.mm(h, self.WG) + self.bG\n",
    "        # if k-sparse, apply\n",
    "        pre_act_v = self.k_sparse(pre_act_v, self.k)\n",
    "        return self.activation(pre_act_v, self.unit)\n",
    "    \n",
    "    def sample_h_given_v(self, v):\n",
    "        h_mean = self.propup(v)\n",
    "        return self.sample(h_mean, self.unit)\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        v_mean = self.propdown(h)\n",
    "        return self.sample(v_mean, self.unit)\n",
    "    \n",
    "    def get_wake_derivs(self):\n",
    "        # get delta by propagating down with output\n",
    "        delta = self.propdown(self.output)\n",
    "        \n",
    "        # get wake derivatives\n",
    "        dWG = T.div(T.mm(self.output.transpose(0,1), (self.input - delta)),\n",
    "                    T.as_tensor(self.input.shape[0], dtype=floatX))\n",
    "        dbG = T.mean((self.input - delta), 0)\n",
    "        \n",
    "        # if top_layer, no WG derivs\n",
    "        if self.top_layer:\n",
    "            return [dbG]\n",
    "        else:\n",
    "            return [dWG, dbG]\n",
    "        \n",
    "    def get_sleep_derivs(self):\n",
    "        # if top_layer, no sleep derivs\n",
    "        if self.top_layer:\n",
    "            return []\n",
    "        \n",
    "        # get psi by propagating up with reconstr\n",
    "        psi = self.propup(self.reconstr)\n",
    "        \n",
    "        # get sleep derivatives\n",
    "        dWR = T.div(T.mm(self.reconstr.transpose(0,1), (self.top_down - psi)),\n",
    "                    T.as_tensor(self.reconstr.shape[0], dtype=floatX))\n",
    "        dbR = T.mean((self.top_down - psi), 0)\n",
    "        return [dWR, dbR]\n",
    "    \n",
    "    def switch_awake(self, awake):\n",
    "        # set x,y based on wake or sleep\n",
    "        if awake:\n",
    "            x = self.input\n",
    "            y = self.output\n",
    "        else:\n",
    "            x = self.reconstr\n",
    "            y = self.top_down\n",
    "        return x, y\n",
    "    \n",
    "    def log_prob(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # get activations\n",
    "        q = self.activation(self.propup(x), self.unit)\n",
    "        p = self.activation(self.propdown(y), self.unit)\n",
    "        # compute log probs\n",
    "        if self.unit == 'binary':\n",
    "            log_q = T.sum(T.add(y * T.log(q + 1e-6), (1. - y) * T.log(1. - q + 1e-6)), 1)\n",
    "            log_p = T.sum(T.add(x * T.log(p + 1e-6), (1. - x) * T.log(1. - p + 1e-6)), 1)\n",
    "        elif self.unit == 'gaussian':\n",
    "            log_q = T.sum((-T.log(2. * np.pi) / 2.) - (T.pow((y - q), 2.) / 2.), 1)\n",
    "            log_p = T.sum((-T.log(2. * np.pi) / 2.) - (T.pow((x - p), 2.) / 2.), 1)\n",
    "        return log_q, log_p\n",
    "        \n",
    "    def k_sparse(self, x, k):\n",
    "        # get threshold and repeat across axis 1\n",
    "        if T.gt(k, x.shape[1]):\n",
    "            k = T.tensor(0, dtype=T.int32)\n",
    "        # horrible workaround for torch's dumb indexing bug\n",
    "        k_mask = T.zeros((1, x.shape[1]), dtype=T.uint8)\n",
    "        k_mask[0,-k] = 1\n",
    "        k_mask = k_mask.repeat(x.shape[0], 1)\n",
    "        thr = T.sort(x)[0][k_mask].unsqueeze(1) # T.sort crashes for some reason\n",
    "        thr = thr.repeat(1, x.shape[1])\n",
    "        # set values >= thr to x, values <= thr to 0\n",
    "        x[T.lt(T.abs(x), thr)] = 0.\n",
    "        return x\n",
    "    \n",
    "    def set_output(self):\n",
    "        self.output = self.sample_h_given_v(self.input)\n",
    "    \n",
    "    def set_reconstr(self, top_down):\n",
    "        self.top_down = top_down\n",
    "        self.reconstr = self.sample_v_given_h(self.top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzMachine(object):\n",
    "    '''\n",
    "    Helmholtz machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_ins: list of ints, number of inputs for each layer\n",
    "    unit: str, unit type ['binary' (default) or 'gaussian']\n",
    "    k: float, fraction of most active units to keep (less active units set to 0) [default: 0]\n",
    "    batch_size: int, size of mini-batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzMachine\n",
    "    '''\n",
    "    def __init__(self, n_ins, unit='binary', k=T.tensor(0, dtype=T.int32), batch_size=1):\n",
    "        # init vars\n",
    "        self.n_layers = len(n_ins)\n",
    "        self.n_ins = n_ins\n",
    "        self.unit = unit\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        # init first layer input variable\n",
    "        self.v = T.zeros((batch_size, self.n_ins[0]), dtype=floatX)\n",
    "        \n",
    "        # for each layer, append HelmholtzLayer\n",
    "        self.params = []\n",
    "        self.helmholtz_layers = []\n",
    "        for n in range(self.n_layers):\n",
    "            # set bG_1 to True if top layer, False otherwise\n",
    "            is_top_layer = (n == self.n_layers - 1)\n",
    "            # set input_layer\n",
    "            if n == 0:\n",
    "                input_layer = self.v\n",
    "            else:\n",
    "                input_layer = self.helmholtz_layers[-1].output\n",
    "            # set n_out\n",
    "            if is_top_layer:\n",
    "                n_out = 1\n",
    "            else:\n",
    "                n_out = self.n_ins[n+1]\n",
    "            # create helmholtz layer\n",
    "            self.helmholtz_layers.append(HelmholtzLayer(input_layer, \n",
    "                                                         self.n_ins[n], \n",
    "                                                         n_out,\n",
    "                                                         unit=self.unit,\n",
    "                                                         top_layer=is_top_layer,\n",
    "                                                         k=self.k))\n",
    "            # extend params\n",
    "            self.params.extend(self.helmholtz_layers[n].gen_params + self.helmholtz_layers[n].rec_params)\n",
    "            \n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((1,1), dtype=floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def forward(self, v=None):\n",
    "        if v is not None:\n",
    "            self.v = v\n",
    "        for n in range(self.n_layers):\n",
    "            if n == 0:\n",
    "                self.helmholtz_layers[n].input = self.v\n",
    "            else:\n",
    "                self.helmholtz_layers[n].input = self.helmholtz_layers[n-1].output\n",
    "            self.helmholtz_layers[n].set_output()\n",
    "    \n",
    "    def downward(self):\n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((1,1), dtype=floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def model_sample(self):\n",
    "        self.downward()\n",
    "        return self.helmholtz_layers[0].reconstr[0]\n",
    "    \n",
    "    def model_prob(self):\n",
    "        self.downward()\n",
    "        layer0 = self.helmholtz_layers[0]\n",
    "        return layer0.activation(layer0.propdown(layer0.top_down), layer0.unit)\n",
    "    \n",
    "    def free_energy(self, D, awake=T.tensor(1., dtype=T.int32)):\n",
    "        # init FE\n",
    "        FE = 0.\n",
    "            \n",
    "        # compute FE for each layer (log_q of the data is 0)\n",
    "        FE = FE - self.helmholtz_layers[0].log_prob(awake)[1]\n",
    "        for n in range(1, self.n_layers):\n",
    "            log_q = self.helmholtz_layers[n-1].log_prob(awake)[0]\n",
    "            log_p = self.helmholtz_layers[n].log_prob(awake)[1]\n",
    "            FE = FE + (log_q - log_p)\n",
    "        \n",
    "        return T.sum(FE)\n",
    "        \n",
    "    def importance_weighting(self, log_q, log_p):\n",
    "        # from Bornschein et al., 2016\n",
    "        # w = sqrt(p/q)\n",
    "        log_w = (log_p - log_q) / 2.\n",
    "        # w_sum = sum_k(log_pq)\n",
    "        log_w_max = T.max(log_w, 1, keepdims=True)\n",
    "        log_w_sum = T.log(T.sum(T.exp(log_w - log_w_max), 1, keepdims=True)) + log_w_max\n",
    "        # w_norm = w/w_sum\n",
    "        log_w_norm = log_w - log_w_sum\n",
    "        # w = exp(log_w_norm)\n",
    "        return T.exp(log_w_norm)\n",
    "    \n",
    "    def log_likelihood(self, D, awake=T.tensor(1., dtype=T.int32)):\n",
    "        # init log_qs, log_ps\n",
    "        log_qs = []\n",
    "        log_ps = []\n",
    "            \n",
    "        # get log_q, log_p for each layer\n",
    "        for n in range(self.n_layers):\n",
    "            log_q_n, log_p_n = self.helmholtz_layers[n].log_prob(awake)\n",
    "            log_qs.append(log_q_n)\n",
    "            log_ps.append(log_p_n)\n",
    "            \n",
    "        # sum across layers\n",
    "        log_q = T.sum(log_qs, 0)\n",
    "        log_p = T.sum(log_ps, 0)\n",
    "        \n",
    "        # reshape to (batch_size, n_samples)\n",
    "        log_q = T.reshape(log_q, (D.shape[0]//self.n_samples, self.n_samples)) \n",
    "        log_p = T.reshape(log_p, (D.shape[0]//self.n_samples, self.n_samples))\n",
    "        \n",
    "        # get importance weights\n",
    "        w = self.importance_weighting(log_q, log_p)\n",
    "        \n",
    "        # compute log likelihood\n",
    "        log_pq = (log_p - log_q) / 2.\n",
    "        log_w_sum = log_pq - T.log(w + 1e-6)\n",
    "        LL = log_w_sum - T.log(self.n_samples)\n",
    "        \n",
    "        # cost and log likelihood\n",
    "        return LL #T.sum(w * (log_p + log_q))\n",
    "            \n",
    "    def save_model(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "    def show_W(self, layer_idx, W_idx=0, W_type='generative'):\n",
    "        # get WG\n",
    "        if W_type == 'generative':\n",
    "            W = self.helmholtz_layers[layer_idx].WG.T\n",
    "        else: # get WR\n",
    "            W = self.helmholtz_layers[layer_idx].WR\n",
    "        # get img shape\n",
    "        img_shape = [int(np.sqrt(W.shape[0]))]*2\n",
    "        # show W\n",
    "        plt.imshow(W[:,W_idx].reshape(img_shape), cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    def update_params(self, params, gparams, inc_params, lr, m=0.): \n",
    "        #\n",
    "        with T.no_grad():\n",
    "            for param, gparam, inc_param in zip(params, gparams, inc_params):\n",
    "                inc_param *= m\n",
    "                inc_param += lr * gparam\n",
    "                param -= inc_param\n",
    "        \n",
    "    def train(self, train_data, lr=None, awake=1., m=0., k=T.tensor(0, dtype=T.int32),\n",
    "              opts={'optimizer': None, 'momentum': False, 'k_sparse': False, \n",
    "                    'n_samples': 1, 'autograd': True}):\n",
    "        '''WRITEME'''\n",
    "        # set vars\n",
    "        if lr is None and 'lr' in opts:\n",
    "            lr = opts['lr']\n",
    "        if not opts['autograd']:\n",
    "            self.v = train_data\n",
    "        elif 'optimizer' in opts:\n",
    "            optimizer = opts['optimizer']\n",
    "        if opts['k_sparse']:\n",
    "            self.k = T.tensor(k, dtype=T.int32)\n",
    "        if 'n_samples' in opts:\n",
    "            self.n_samples = opts['n_samples']\n",
    "        \n",
    "        # get grad_fn and cost_fn from opts\n",
    "        if 'cost_fn' in opts:\n",
    "            cost_fn = opts['cost_fn']\n",
    "            # get cost\n",
    "            cost = cost_fn(train_data, awake)\n",
    "        else:\n",
    "            cost = None\n",
    "        if 'grad_fn' in opts:\n",
    "            grad_fn = opts['grad_fn']\n",
    "        else:\n",
    "            return cost\n",
    "        \n",
    "        # forward pass if awake, downward pass if asleep\n",
    "        if awake:\n",
    "            helm.forward(train_data)\n",
    "        else:\n",
    "            helm.downward()\n",
    "        \n",
    "        # get wake/sleep derivatives\n",
    "        for n in range(self.n_layers):\n",
    "            # set params\n",
    "            if awake:\n",
    "                params = self.helmholtz_layers[n].gen_params\n",
    "                inc_params = self.helmholtz_layers[n].inc_params[:2]\n",
    "            else:\n",
    "                params = self.helmholtz_layers[n].rec_params\n",
    "                inc_params = self.helmholtz_layers[n].inc_params[2:]\n",
    "            if opts['autograd']: # automatic derivatives\n",
    "                grad_fn(train_data, awake).backward(retain_graph=True)\n",
    "                # update params if no optimizer\n",
    "                if 'optimizer' not in opts:\n",
    "                    gparams = T.autograd.grad(grad_fn(train_data, awake), params, retain_graph=True)\n",
    "                    self.update_params(params, gparams, inc_params, lr, m)\n",
    "            elif awake: # manual wake derivatives\n",
    "                gparams = self.helmholtz_layers[n].get_wake_derivs()\n",
    "                # update params\n",
    "                self.update_params(params, gparams, inc_params, lr, m)\n",
    "            elif not awake: # manual sleep derivatives\n",
    "                gparams = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "                # update params\n",
    "                self.update_params(params, gparams, inc_params, lr, m)\n",
    "        \n",
    "        # take step with optimizer\n",
    "        if opts['autograd'] and 'optimizer' in opts:\n",
    "            optimizer.step()\n",
    "                    \n",
    "        return cost\n",
    "        \n",
    "    def set_opts(self, batch_size, grad_fn='free_energy', cost_fn='free_energy',\n",
    "                       opts={'momentum': False, 'k_sparse': False, 'lr': 1e-3,\n",
    "                             'n_samples': 1, 'autograd': True}):\n",
    "        '''WRITEME\n",
    "        Create training function with given update rule, momentum, and sparsity\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: theano matrix, training data to use\n",
    "        batch_size: int, size of each training mini-batch\n",
    "        cost_type: str, cost type to use for update rule ['free_energy' (default) or 'log_likelihood']\n",
    "        opts: dict, options for momentum, sparsity, importance sampling, and automatic differentiation\n",
    "            'momentum': bool, True/False use momentum [default: False]\n",
    "            'k_sparse': bool, True/False to do k-sparse (includes new input for train_fn) [default: False]\n",
    "            'n_samples': int, number of samples to use for importance sampling [default: 1] (cost_type='loglikelihood')\n",
    "            'autograd': bool, True/False use automatic differentiation [default: True] (cost_type='free_energy')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_fn: theano function, training function with following inputs\n",
    "            idx: int, index of training mini-batch (i.e. train_data[idx*batch_size:(idx+1)*batch_size])\n",
    "            awake: bool, True/False awake for wake/sleep algorithm\n",
    "            m: float, momentum to apply (if opts['momentum'] == True)\n",
    "            k: float, fraction of most active units to keep (less active set to 0; if opts['k_sparse'] == True)\n",
    "            *lr: float(s), learning rates for each layer\n",
    "            \n",
    "        Note\n",
    "        ----\n",
    "        In order to use k-sparse, when initializing HelmholtzMachine set k=T.scalar('k').\n",
    "        '''\n",
    "        # init vars\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # init momentum, k_sparse, lr\n",
    "        if 'momentum' not in opts:\n",
    "            opts['momentum'] = False\n",
    "        if 'k_sparse' not in opts:\n",
    "            opts['k_sparse'] = False\n",
    "        if 'lr' not in opts:\n",
    "            opts['lr'] = 1e-3\n",
    "        \n",
    "        # n_samples (add to self so log_likelihood() has access)\n",
    "        if 'n_samples' in opts and (grad_fn == 'log_likelihood' or cost_fn == 'log_likelihood'):\n",
    "            self.n_samples = opts['n_samples']\n",
    "        else:\n",
    "            self.n_samples = 1\n",
    "            \n",
    "        # get gradient function\n",
    "        if grad_fn == 'free_energy':\n",
    "            opts['grad_fn'] = self.free_energy\n",
    "        elif grad_fn == 'log_likelihood':\n",
    "            opts['cost_fn'] = self.log_likelihood\n",
    "            opts['autograd'] = True\n",
    "        \n",
    "        # set optimizer\n",
    "        if opts['autograd']:\n",
    "            opts['optimizer'] = T.optim.SGD(helm.params, lr=opts['lr'])\n",
    "            opts['optimizer'].zero_grad()\n",
    "        \n",
    "        # get cost function\n",
    "        if cost_fn == 'free_energy':\n",
    "            opts['cost_fn'] = self.free_energy\n",
    "        elif cost_fn == 'log_likelihood':\n",
    "            opts['cost_fn'] = self.log_likelihood\n",
    "        \n",
    "        return opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bar data\n",
    "data = np.zeros((1000, 3, 3))\n",
    "for n in range(data.shape[0]):\n",
    "    # Horizontal bars 1/3, vertical bars 2/3\n",
    "    if np.random.rand() > (2./3.): \n",
    "        data[n, np.random.randint(data.shape[1]), :] = 1.\n",
    "    else:\n",
    "        data[n, :, np.random.randint(data.shape[2])] = 1.\n",
    "data = np.reshape(data, (data.shape[0], -1))\n",
    "data = T.as_tensor(data, dtype=floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helm = HelmholtzMachine([9,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts = helm.set_opts(1, opts={'autograd': False, 'lr': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3437, grad_fn=<SumBackward0>)\r"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "for epoch in range(1000):\n",
    "    # train\n",
    "    helm.train(data[None,0], awake=1, lr=1e-2, opts=opts)\n",
    "    cost.append(helm.train(data[None,0], awake=0, lr=1e-2, opts=opts))\n",
    "    # print cost\n",
    "    print(cost[-1], end='\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
