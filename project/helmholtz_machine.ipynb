{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d, conv2d_transpose\n",
    "from theano.tensor.signal import pool\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "theano.config.compute_test_value = 'off'\n",
    "theano.config.floatX = 'float32'\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create shared variables for using gpu\n",
    "def shared_dataset(data, borrow=True, data_types=['float32','int32']):\n",
    "    if type(data) is not list:\n",
    "        data = list(data)\n",
    "    output = []\n",
    "    for i, x in enumerate(data):\n",
    "        output.append(theano.shared(np.asarray(x, dtype=data_types[i]), borrow=borrow))\n",
    "    return output\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    # get path/file for dataset\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the current directory.\n",
    "        new_path = os.path.join(os.curdir, dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "    # download from website\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "    # load from pickle\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # set test/valid/train sets\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    # combine datasets\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzLayer(object):\n",
    "    '''\n",
    "    Helmholtz layer for Helmholtz Machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input: theano matrix, input to layer (data or output of previous layer)\n",
    "    n_in: int, number of input units\n",
    "    n_out: int, number of hidden units\n",
    "    unit: str, hidden unit type ['binary' (default) or 'gaussian']\n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    top_layer: bool, True/False layer is top layer\n",
    "    sparse_thr: float, fraction of most active units for K-max WTA; less active units are set to 0\n",
    "        [default: 0., no units set to 0]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzLayer\n",
    "    '''\n",
    "    def __init__(self, input, n_in, n_out, unit='binary', th_rng=None, top_layer=False, sparse_thr=0.):\n",
    "        # init vars\n",
    "        self.input = input\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.unit = unit\n",
    "        # init th_rng if None\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(1)\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.top_layer = top_layer\n",
    "        self.sparse_thr = sparse_thr\n",
    "        \n",
    "        # recognition weights\n",
    "        self.WR = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_in,n_out)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WR')\n",
    "        # recognition biases\n",
    "        self.bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX), \n",
    "                                borrow=True, name='bR')\n",
    "        # generative weights\n",
    "        self.WG = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_out,n_in)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WG')\n",
    "        # generative biases\n",
    "        self.bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                borrow=True, name='bG')\n",
    "        \n",
    "        # momentum\n",
    "        self.inc_WR = theano.shared(np.zeros((n_in,n_out), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WR')\n",
    "        self.inc_bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bR')\n",
    "        self.inc_WG = theano.shared(np.zeros((n_out,n_in), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WG') \n",
    "        self.inc_bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bG')\n",
    "        \n",
    "        # if top_layer, remove shared WR, bR, WG\n",
    "        if self.top_layer:\n",
    "            self.WR = T.zeros_like(self.WR)\n",
    "            self.bR = T.zeros_like(self.bR)\n",
    "            self.WG = T.zeros_like(self.WG)\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.bG]\n",
    "            self.rec_params = []\n",
    "            self.inc_params = [self.inc_bG]\n",
    "        else:\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.WG, self.bG]\n",
    "            self.rec_params = [self.WR, self.bR]\n",
    "            self.inc_params = [self.inc_WG, self.inc_bG, self.inc_WR, self.inc_bR]\n",
    "        \n",
    "        # set output\n",
    "        self.output = self.sample_h_given_v(self.input)\n",
    "        \n",
    "        # init reconstr, top_down\n",
    "        self.reconstr = None\n",
    "        self.top_down = None\n",
    "        \n",
    "    def activation(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = T.nnet.sigmoid(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = u\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def sample(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = self.th_rng.binomial(size=u.shape, n=1, p=u, dtype=theano.config.floatX)\n",
    "        elif unit == 'gaussian':\n",
    "            y = T.add(u, self.th_rng.normal(u.shape, std=1., dtype=theano.config.floatX))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def prob(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            p = u\n",
    "        elif unit == 'gaussian':\n",
    "            p = (1./T.sqrt(2. * np.pi)) * T.exp(-T.sqr(u)/2.)\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return p\n",
    "    \n",
    "    def propup(self, v):\n",
    "        pre_act_h = T.dot(v, self.WR) + self.bR\n",
    "        return self.activation(pre_act_h, self.unit)\n",
    "    \n",
    "    def propdown(self, h):\n",
    "        pre_act_v = T.dot(h, self.WG) + self.bG\n",
    "        return self.activation(pre_act_v, self.unit)\n",
    "    \n",
    "    def sample_h_given_v(self, v):\n",
    "        h_mean = self.propup(v)\n",
    "        # if sparse_thr, apply\n",
    "        h_mean = self.sparse_threshold(h_mean, self.sparse_thr)\n",
    "        return self.sample(h_mean, self.unit)\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        v_mean = self.propdown(h)\n",
    "        # if sparse_thr, apply\n",
    "        v_mean = self.sparse_threshold(v_mean, self.sparse_thr)\n",
    "        return self.sample(v_mean, self.unit)\n",
    "    \n",
    "    def get_wake_derivs(self):\n",
    "        # get delta by propagating down with output\n",
    "        delta = self.propdown(self.output)\n",
    "        \n",
    "        # get wake derivatives\n",
    "        dWG = T.dot(self.output.T, (self.input - delta))/T.cast(self.input.shape[0], dtype=theano.config.floatX)\n",
    "        dbG = T.mean((self.input - delta), axis=0)\n",
    "        \n",
    "        # if top_layer, no WG derivs\n",
    "        if self.top_layer:\n",
    "            return [dbG]\n",
    "        else:\n",
    "            return [dWG, dbG]\n",
    "        \n",
    "    def get_sleep_derivs(self):\n",
    "        # if top_layer, no sleep derivs\n",
    "        if self.top_layer:\n",
    "            return []\n",
    "        \n",
    "        # get psi by propagating up with reconstr\n",
    "        psi = self.propup(self.reconstr)\n",
    "        \n",
    "        # get sleep derivatives\n",
    "        dWR = T.dot(self.reconstr.T, (self.top_down - psi))/T.cast(self.reconstr.shape[0], dtype=theano.config.floatX)\n",
    "        dbR = T.mean((self.top_down - psi), axis=0)\n",
    "        return [dWR, dbR]\n",
    "    \n",
    "    def switch_awake(self, awake):\n",
    "        # set x,y based on wake or sleep\n",
    "        x = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.input) + awake,\n",
    "                                                       self.input, self.reconstr))\n",
    "        y = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.output) + awake,\n",
    "                                                       self.output, self.top_down))\n",
    "        return x, y\n",
    "    \n",
    "    def prob_qp(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # compute q and p\n",
    "        q = self.prob(self.propup(x), self.unit)\n",
    "        p = self.prob(self.propdown(y), self.unit)\n",
    "        return q, p\n",
    "    \n",
    "    def log_prob(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # get probs\n",
    "        q, p = self.prob_qp(awake)\n",
    "        # compute log probs\n",
    "        log_q = T.sum(T.add(y * T.log(q + 1e-6), (1. - y) * T.log(1. - q + 1e-6)), axis=1)\n",
    "        log_p = T.sum(T.add(x * T.log(p + 1e-6), (1. - x) * T.log(1. - p + 1e-6)), axis=1)\n",
    "        return log_q, log_p\n",
    "    \n",
    "    def sparsity(self, t, s, awake, params):\n",
    "        q, p = self.prob_qp(awake)\n",
    "        q = T.mean(q, axis=0)\n",
    "        p = T.mean(p, axis=0)\n",
    "        cost = s * T.add(T.sum(t * T.log(q + 1e-6) + (1. - t) * T.log(1. - q + 1e-6)),\n",
    "                         T.sum(t * T.log(p + 1e-6) + (1. - t) * T.log(1. - p + 1e-6)))\n",
    "        return T.grad(-cost, params)\n",
    "    \n",
    "    def sparse_threshold(self, x, thr):\n",
    "        # get threshold and repeat across axis=1\n",
    "        thr = T.sort(T.abs_(x))[:, -T.cast(thr * x.shape[1], 'int32')].dimshuffle((0,'x'))\n",
    "        thr = T.repeat(thr, x.shape[1], axis=1)\n",
    "        # set values >= thr to x, values <= thr to 0\n",
    "        return T.switch(T.ge(T.abs_(x), thr), x, T.zeros_like(x))\n",
    "    \n",
    "    def set_reconstr(self, top_down):\n",
    "        self.top_down = top_down\n",
    "        self.reconstr = self.sample_v_given_h(self.top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzMachine(object):\n",
    "    '''\n",
    "    Helmholtz machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_ins: list of ints, number of inputs for each layer\n",
    "    unit: str, unit type ['binary' (default) or 'gaussian']\n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzMachine\n",
    "    '''\n",
    "    def __init__(self, n_ins, unit='binary', th_rng=None, sparse_thr=0.):\n",
    "        # init vars\n",
    "        self.n_layers = len(n_ins)\n",
    "        self.n_ins = n_ins\n",
    "        self.unit = unit\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(np.random.randint(2**30))\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.sparse_thr = sparse_thr\n",
    "            \n",
    "        # init first layer input variable\n",
    "        self.v = T.matrix('v')\n",
    "        \n",
    "        # for each layer, append HelmholtzLayer\n",
    "        self.helmholtz_layers = []\n",
    "        for n in range(self.n_layers):\n",
    "            # set bG_1 to True if top layer, False otherwise\n",
    "            is_top_layer = (n == self.n_layers - 1)\n",
    "            # set input_layer\n",
    "            if n == 0:\n",
    "                input_layer = self.v\n",
    "            else:\n",
    "                input_layer = self.helmholtz_layers[-1].output\n",
    "            # set n_out\n",
    "            if is_top_layer:\n",
    "                n_out = 1\n",
    "            else:\n",
    "                n_out = self.n_ins[n+1]\n",
    "            # create helmholtz layer\n",
    "            self.helmholtz_layers.append(HelmholtzLayer(input_layer, \n",
    "                                                         self.n_ins[n], \n",
    "                                                         n_out,\n",
    "                                                         unit=self.unit,\n",
    "                                                         th_rng=self.th_rng,\n",
    "                                                         top_layer=is_top_layer,\n",
    "                                                         sparse_thr=self.sparse_thr))\n",
    "            \n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((1,1), dtype=theano.config.floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def model_sample(self):\n",
    "        return self.helmholtz_layers[0].reconstr[0]\n",
    "    \n",
    "    def model_prob(self):\n",
    "        layer0 = self.helmholtz_layers[0]\n",
    "        return layer0.prob(layer0.propdown(layer0.top_down), layer0.unit)\n",
    "    \n",
    "    def free_energy_part(self, q, p):\n",
    "        return T.sum(T.add(q * (T.log(q + 1e-6) - T.log(p + 1e-6)),\n",
    "                           (1. - q) * (T.log(1. - q + 1e-6) - T.log(1. - p + 1e-6))))\n",
    "    \n",
    "    def free_energy(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        FEs = []\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # compute FE for each layer\n",
    "        FEs.append(self.free_energy_part(D, self.helmholtz_layers[0].prob_qp(awake)[1]))\n",
    "        for n in range(1, self.n_layers):\n",
    "            FEs.append(self.free_energy_part(self.helmholtz_layers[n-1].prob_qp(awake)[0],\n",
    "                                             self.helmholtz_layers[n].prob_qp(awake)[1]))\n",
    "        \n",
    "        # return function that can compute FE for given data\n",
    "        if return_fn:\n",
    "            return theano.function([D], T.sum(FEs), givens={self.v: D})\n",
    "        else:\n",
    "            return T.grad(-T.sum(FEs), params)\n",
    "       \n",
    "    def importance_weighting(self, log_q, log_p):\n",
    "        # from Bornschein et al., 2016\n",
    "        # w = sqrt(p/q)\n",
    "        log_w = (log_p - log_q) / 2.\n",
    "        # w_sum = sum_k(log_pq)\n",
    "        log_w_max = T.max(log_w, axis=1, keepdims=True)\n",
    "        log_w_sum = T.log(T.sum(T.exp(log_w - log_w_max), axis=1, keepdims=True)) + log_w_max\n",
    "        # w_norm = w/w_sum\n",
    "        log_w_norm = log_w - log_w_sum\n",
    "        # w = exp(log_w_norm)\n",
    "        return T.exp(log_w_norm)\n",
    "    \n",
    "    def log_likelihood(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        log_qs = []\n",
    "        log_ps = []\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # get log_q, log_p for each layer\n",
    "        for n in range(self.n_layers):\n",
    "            log_q_n, log_p_n = self.helmholtz_layers[n].log_prob(awake)\n",
    "            log_qs.append(log_q_n)\n",
    "            log_ps.append(log_p_n)\n",
    "            \n",
    "        # sum across layers\n",
    "        log_q = T.sum(log_qs, axis=0)\n",
    "        log_p = T.sum(log_ps, axis=0)\n",
    "        \n",
    "        # reshape to (batch_size, n_samples)\n",
    "        log_q = T.reshape(log_q, (D.shape[0]//self.n_samples, self.n_samples)) \n",
    "        log_p = T.reshape(log_p, (D.shape[0]//self.n_samples, self.n_samples))\n",
    "        \n",
    "        # get importance weights\n",
    "        w = self.importance_weighting(log_q, log_p)\n",
    "        \n",
    "        # compute log likelihood\n",
    "        log_pq = (log_p - log_q) / 2.\n",
    "        w_norm = T.log(w + 1e-6) - log_pq\n",
    "        LL = w_norm - T.log(self.n_samples)\n",
    "        \n",
    "        # cost\n",
    "        cost = T.sum(w * (log_p + log_q))\n",
    "            \n",
    "        # return theano function or LL\n",
    "        if return_fn:\n",
    "            return theano.function([D], -LL, givens={self.v: D})\n",
    "        else:\n",
    "            return T.grad(cost, params)\n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "    def train_function(self, train_data, batch_size, cost_type='free_energy', \n",
    "                       opts={'momentum': False, 'sparsity': False, 'sparsity_cost': 1., \n",
    "                             'sparsity_target': 0.001, 'n_samples': 1, 'auto_derivs': True}):\n",
    "        '''\n",
    "        Create training function with given update rule, momentum, and sparsity\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: theano matrix, training data to use\n",
    "        batch_size: int, size of each training mini-batch\n",
    "        cost_type: str, cost type to use for update rule ['free_energy' (default) or 'log_likelihood']\n",
    "        opts: dict, options for momentum, sparsity, importance sampling, and automatic differentiation\n",
    "            'momentum': bool, True/False use momentum [default: False]\n",
    "            'sparsity': bool, True/False use sparsity [default: False]\n",
    "            'sparsity_cost': float, scalar to multiply sparsity cost [default: 1.]\n",
    "            'sparsity_target': float, target sparsity for hidden activities [default: 0.001]\n",
    "            'n_samples': int, number of samples to use for importance sampling [default: 1] (cost_type='loglikelihood')\n",
    "            'auto_derivs': bool, True/False use automatic differentiation [default: True] (cost_type='free_energy')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_fn: theano function, training function with following inputs\n",
    "            idx: int, index of training mini-batch (i.e. train_data[idx*batch_size:(idx+1)*batch_size])\n",
    "            awake: bool, True/False awake for wake/sleep algorithm\n",
    "            m: float, momentum to apply (only if opts['momentum'] == True)\n",
    "            *lr: float(s), learning rates for each layer\n",
    "        '''\n",
    "        # init vars\n",
    "        awake = T.iscalar('awake').astype(theano.config.floatX)\n",
    "        lr = [T.scalar('lr_' + str(n)) / batch_size for n in range(self.n_layers)]\n",
    "        idx = T.iscalar('idx')\n",
    "        v = train_data[idx * batch_size:(idx + 1) * batch_size]\n",
    "        \n",
    "        # momentum options\n",
    "        if 'momentum' in opts:\n",
    "            momentum = opts['momentum']\n",
    "        else:\n",
    "            momentum = False\n",
    "        if momentum:\n",
    "            m = T.scalar('m')\n",
    "        else:\n",
    "            m = 0.\n",
    "        # sparsity options\n",
    "        if 'sparsity' in opts:\n",
    "            sparsity = opts['sparsity']\n",
    "        else:\n",
    "            sparsity = False\n",
    "        if 'spasity_cost' in opts:\n",
    "            sparsity_cost = opts['sparsity_cost']\n",
    "        elif sparsity:\n",
    "            sparsity_cost = 1.\n",
    "        if 'sparsity_target' in opts:\n",
    "            sparsity_target = opts['sparsity_target']\n",
    "        else:\n",
    "            sparsity_target = 0.001\n",
    "        # n_samples (add to self so log_likelihood() has access)\n",
    "        if 'n_samples' in opts and cost_type == 'log_likelihood':\n",
    "            self.n_samples = opts['n_samples']\n",
    "        else:\n",
    "            self.n_samples = 1\n",
    "        # automatic differentiation\n",
    "        if 'auto_derivs' in opts:\n",
    "            auto_derivs = opts['auto_derivs']\n",
    "        else:\n",
    "            auto_derivs = True\n",
    "            \n",
    "        # get gradient function\n",
    "        if cost_type == 'free_energy':\n",
    "            print('Cost: free energy (automatic differentiation: %s)' % (auto_derivs))\n",
    "            grad_fn = self.free_energy\n",
    "        elif cost_type == 'log_likelihood':\n",
    "            print('Cost: log likelihood (n_samples: %d)' % (self.n_samples))\n",
    "            v = T.repeat(v, self.n_samples, axis=0)\n",
    "            grad_fn = self.log_likelihood\n",
    "            auto_derivs = True\n",
    "        \n",
    "        # parameter updates for theano function\n",
    "        updates = []\n",
    "        for n in range(self.n_layers):\n",
    "            if auto_derivs: # automatic derivatives\n",
    "                wake_derivs = grad_fn(v, awake, self.helmholtz_layers[n].gen_params)\n",
    "                sleep_derivs = grad_fn(v, awake, self.helmholtz_layers[n].rec_params)\n",
    "            else: # manual derivatives\n",
    "                wake_derivs = self.helmholtz_layers[n].get_wake_derivs()\n",
    "                sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "                \n",
    "            # sparsity\n",
    "            if sparsity:\n",
    "                wake_sparse = self.helmholtz_layers[n].sparsity(sparsity_target, sparsity_cost,\n",
    "                                                                awake, self.helmholtz_layers[n].gen_params)\n",
    "                wake_derivs = [g + s for g, s in zip(wake_derivs, wake_sparse)]\n",
    "                sleep_sparse = self.helmholtz_layers[n].sparsity(sparsity_target, sparsity_cost,\n",
    "                                                                 awake, self.helmholtz_layers[n].rec_params)\n",
    "                sleep_derivs = [g + s for g, s in zip(sleep_derivs, sleep_sparse)]\n",
    "                \n",
    "            # wake phase\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].gen_params, wake_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[:2]):\n",
    "                updates.append((param, param + awake * lr[n] * gparam + inc_param * m))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, awake * (inc_param * m + lr[n] * gparam)))\n",
    "                \n",
    "            # sleep phase\n",
    "            sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].rec_params, sleep_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[2:]):\n",
    "                updates.append((param, param + (1. - awake) * lr[n] * gparam + inc_param * m))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, (1. - awake) * (inc_param * m + lr[n] * gparam)))\n",
    "        \n",
    "        # create train_fn\n",
    "        if momentum:\n",
    "            inputs = [idx, awake, m] + lr\n",
    "        else:\n",
    "            inputs = [idx, awake] + lr\n",
    "        train_fn = theano.function(inputs, [], updates=updates, givens={self.v: v})\n",
    "           \n",
    "        return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('mnist.pkl.gz')[0][0]\n",
    "# data = T.gt(data, 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bar data\n",
    "data = np.zeros((1000, 3, 3), dtype=theano.config.floatX)\n",
    "for n in range(data.shape[0]):\n",
    "    # Horizontal bars 1/3, vertical bars 2/3\n",
    "    if np.random.rand() > (2./3.): \n",
    "        data[n, np.random.randint(data.shape[1]), :] = 1.\n",
    "    else:\n",
    "        data[n, :, np.random.randint(data.shape[2])] = 1.\n",
    "data = np.reshape(data, (data.shape[0], -1))\n",
    "data = T.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: free energy (automatic differentiation: False)\n"
     ]
    }
   ],
   "source": [
    "# initialize helmholtz machine\n",
    "helm = HelmholtzMachine([784, 500, 10], unit='binary', sparse_thr=0.2) #[9,6,1]\n",
    "# create training and cost monitoring functions\n",
    "batch_size = 10\n",
    "opts = {'momentum': True, 'sparsity': False, 'auto_derivs': False} #, 'sparsity_cost': 0.5, 'sparsity_target': 0.001, 'n_samples': 10}\n",
    "train_fn = helm.train_function(data, batch_size, cost_type='free_energy', opts=opts)\n",
    "cost_fn = helm.free_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 18 (10.00%): 581403.6250'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD+QKcIPZIrwA5n6f7wEXXOJeHbsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD8CAYAAAC/1zkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5XBIDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "lr = [1e-3]*helm.n_layers\n",
    "n_batches = data.eval().shape[0]//batch_size\n",
    "monitor = n_batches//10 # monitor every 10%\n",
    "cost_history = []\n",
    "im_size = (28,28)\n",
    "\n",
    "# train in epochs\n",
    "for epoch in range(50):\n",
    "    # momentum\n",
    "    if epoch < 5:\n",
    "        m = 0.5\n",
    "    else:\n",
    "        m = 0.9\n",
    "    cost = []\n",
    "    # for each batch_size within n_batches, train\n",
    "    for n in range(n_batches):\n",
    "        train_fn(n, True, m, *lr) # wake phase\n",
    "        train_fn(n, False, m, *lr) # sleep phase\n",
    "        \n",
    "        # monitor samples\n",
    "        if n % monitor == 0:\n",
    "            # get cost for random set of mini-batches\n",
    "            rand_idxs = np.random.permutation(data.eval().shape[0])\n",
    "            cost.append(cost_fn(data[rand_idxs[:monitor*batch_size]].eval()))\n",
    "            \n",
    "            # print progress\n",
    "            clear_output(wait=True)\n",
    "            display('Epoch %d (%0.2f%%): %0.4f' % (epoch, 100. * n/n_batches, np.mean(cost)))\n",
    "            \n",
    "            # plot sample (sigmoid) from model\n",
    "            plt.imshow(helm.model_prob().eval().reshape(im_size), cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            # plot cost history\n",
    "            plt.plot(cost_history)\n",
    "            plt.show()\n",
    "            \n",
    "    # append cost history\n",
    "    cost_history.append(np.mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show samples (sigmoid) from model\n",
    "plt.imshow(helm.model_prob().eval().reshape(im_size), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
