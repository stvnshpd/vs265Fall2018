{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d, conv2d_transpose\n",
    "from theano.tensor.signal import pool\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "theano.config.compute_test_value = 'off'\n",
    "theano.config.floatX = 'float32'\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create shared variables for using gpu\n",
    "def shared_dataset(data, borrow=True, data_types=['float32','int32']):\n",
    "    if type(data) is not list:\n",
    "        data = list(data)\n",
    "    output = []\n",
    "    for i, x in enumerate(data):\n",
    "        output.append(theano.shared(np.asarray(x, dtype=data_types[i]), borrow=borrow))\n",
    "    return output\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    # get path/file for dataset\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the current directory.\n",
    "        new_path = os.path.join(os.curdir, dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "    # download from website\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "    # load from pickle\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # set test/valid/train sets\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    # combine datasets\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzLayer(object):\n",
    "    '''\n",
    "    Helmholtz layer for Helmholtz Machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input: theano matrix, input to layer (data or output of previous layer)\n",
    "    n_in: int, number of input units\n",
    "    n_out: int, number of hidden units\n",
    "    unit: str, hidden unit type ['binary' (default) or 'gaussian']\n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    top_layer: bool, True/False layer is top layer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzLayer\n",
    "    '''\n",
    "    def __init__(self, input, n_in, n_out, unit='binary', th_rng=None, top_layer=False):\n",
    "        # init vars\n",
    "        self.input = input\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.unit = unit\n",
    "        # init th_rng if None\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(1)\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.top_layer = top_layer\n",
    "        \n",
    "        # recognition weights\n",
    "        self.WR = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_in,n_out)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WR')\n",
    "        # recognition biases\n",
    "        self.bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX), \n",
    "                                borrow=True, name='bR')\n",
    "        # generative weights\n",
    "        self.WG = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_out,n_in)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WG')\n",
    "        # generative biases\n",
    "        self.bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                borrow=True, name='bG')\n",
    "        \n",
    "        # momentum\n",
    "        self.inc_WR = theano.shared(np.zeros((n_in,n_out), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WR')\n",
    "        self.inc_bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bR')\n",
    "        self.inc_WG = theano.shared(np.zeros((n_out,n_in), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WG') \n",
    "        self.inc_bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bG')\n",
    "        \n",
    "        # if top_layer, remove shared WR, bR, WG\n",
    "        if self.top_layer:\n",
    "            self.WR = T.zeros_like(self.WR)\n",
    "            self.bR = T.zeros_like(self.bR)\n",
    "            self.WG = T.zeros_like(self.WG)\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.bG]\n",
    "            self.rec_params = []\n",
    "            self.inc_params = [self.inc_bG]\n",
    "        else:\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.WG, self.bG]\n",
    "            self.rec_params = [self.WR, self.bR]\n",
    "            self.inc_params = [self.inc_WG, self.inc_bG, self.inc_WR, self.inc_bR]\n",
    "        \n",
    "        # set output\n",
    "        self.output = self.sample_h_given_v(self.input)\n",
    "        \n",
    "        # init reconstr, top_down\n",
    "        self.reconstr = None\n",
    "        self.top_down = None\n",
    "        \n",
    "    def activation(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = T.nnet.sigmoid(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = u\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def sample(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = self.th_rng.binomial(size=u.shape, n=1, p=u, dtype=theano.config.floatX)\n",
    "        elif unit == 'gaussian':\n",
    "            y = T.add(u, self.th_rng.normal(u.shape, std=1., dtype=theano.config.floatX))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def prob(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            p = u\n",
    "        elif unit == 'gaussian':\n",
    "            p = (1./T.sqrt(2. * np.pi)) * T.exp(-T.sqr(u)/2.)\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return p\n",
    "    \n",
    "    def propup(self, v):\n",
    "        pre_act_h = T.dot(v, self.WR) + self.bR\n",
    "        return self.activation(pre_act_h, self.unit)\n",
    "    \n",
    "    def propdown(self, h):\n",
    "        pre_act_v = T.dot(h, self.WG) + self.bG\n",
    "        return self.activation(pre_act_v, self.unit)\n",
    "    \n",
    "    def sample_h_given_v(self, v):\n",
    "        h_mean = self.propup(v)\n",
    "        return self.sample(h_mean, self.unit)\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        v_mean = self.propdown(h)\n",
    "        return self.sample(v_mean, self.unit)\n",
    "    \n",
    "    def get_wake_derivs(self):\n",
    "        # get delta by propagating down with output\n",
    "        delta = self.propdown(self.output)\n",
    "        \n",
    "        # get wake derivatives\n",
    "        dWG = T.dot(self.output.T, (self.input - delta))\n",
    "        dbG = T.mean((self.input - delta), axis=0)\n",
    "        \n",
    "        # if top_layer, no WG derivs\n",
    "        if self.top_layer:\n",
    "            return [dbG]\n",
    "        else:\n",
    "            return [dWG, dbG]\n",
    "        \n",
    "    def get_sleep_derivs(self):\n",
    "        # if top_layer, no sleep derivs\n",
    "        if self.top_layer:\n",
    "            return []\n",
    "        \n",
    "        # get psi by propagating up with reconstr\n",
    "        psi = self.propup(self.reconstr)\n",
    "        \n",
    "        # get sleep derivatives\n",
    "        dWR = T.dot(self.reconstr.T, (self.top_down - psi))\n",
    "        dbR = T.mean((self.top_down - psi), axis=0)\n",
    "        return [dWR, dbR]\n",
    "    \n",
    "    def switch_awake(self, awake):\n",
    "        # set x,y based on wake or sleep\n",
    "        x = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.input) + awake,\n",
    "                                                       self.input, self.reconstr))\n",
    "        y = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.output) + awake,\n",
    "                                                       self.output, self.top_down))\n",
    "        return x, y\n",
    "    \n",
    "    def prob_qp(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # compute q and p\n",
    "        q = self.prob(self.propup(x), self.unit)\n",
    "        p = self.prob(self.propdown(y), self.unit)\n",
    "        return q, p\n",
    "    \n",
    "    def log_prob(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # get probs\n",
    "        q, p = self.prob_qp(awake)\n",
    "        # compute log probs\n",
    "        log_q = T.sum(T.add(y * T.log(q + 1e-6), (1. - y) * T.log(1. - q + 1e-6)), axis=1)\n",
    "        log_p = T.sum(T.add(x * T.log(p + 1e-6), (1. - x) * T.log(1. - p + 1e-6)), axis=1)\n",
    "        return log_q, log_p\n",
    "    \n",
    "    def sparsity(self, t, s, awake, params):\n",
    "        q, p = self.prob_qp(awake)\n",
    "        q = T.mean(q, axis=0)\n",
    "        p = T.mean(p, axis=0)\n",
    "        cost = s * T.add(T.sum(t * T.log(q + 1e-6) + (1. - t) * T.log(1. - q + 1e-6)),\n",
    "                         T.sum(t * T.log(p + 1e-6) + (1. - t) * T.log(1. - p + 1e-6)))\n",
    "        return T.grad(-cost, params)\n",
    "    \n",
    "    def set_reconstr(self, top_down):\n",
    "        self.top_down = top_down\n",
    "        self.reconstr = self.sample_v_given_h(self.top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzMachine(object):\n",
    "    '''\n",
    "    Helmholtz machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_ins: list of ints, number of inputs for each layer\n",
    "    unit: str, unit type ['binary' (default) or 'gaussian']\n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzMachine\n",
    "    '''\n",
    "    def __init__(self, n_ins, unit='binary', th_rng=None):\n",
    "        # init vars\n",
    "        self.n_layers = len(n_ins)\n",
    "        self.n_ins = n_ins\n",
    "        self.unit = unit\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(np.random.randint(2**30))\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "            \n",
    "        # init first layer input variable\n",
    "        self.v = T.matrix('v')\n",
    "        \n",
    "        # for each layer, append HelmholtzLayer\n",
    "        self.helmholtz_layers = []\n",
    "        for n in range(self.n_layers):\n",
    "            # set bG_1 to True if top layer, False otherwise\n",
    "            is_top_layer = (n == self.n_layers - 1)\n",
    "            # set input_layer\n",
    "            if n == 0:\n",
    "                input_layer = self.v\n",
    "            else:\n",
    "                input_layer = self.helmholtz_layers[-1].output\n",
    "            # set n_out\n",
    "            if is_top_layer:\n",
    "                n_out = 1\n",
    "            else:\n",
    "                n_out = self.n_ins[n+1]\n",
    "            # create helmholtz layer\n",
    "            self.helmholtz_layers.append(HelmholtzLayer(input_layer, \n",
    "                                                         self.n_ins[n], \n",
    "                                                         n_out,\n",
    "                                                         unit=self.unit,\n",
    "                                                         th_rng=self.th_rng,\n",
    "                                                         top_layer=is_top_layer))\n",
    "            \n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((1,1), dtype=theano.config.floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def model_sample(self):\n",
    "        return self.helmholtz_layers[0].reconstr[0]\n",
    "    \n",
    "    def model_prob(self):\n",
    "        layer0 = self.helmholtz_layers[0]\n",
    "        return layer0.prob(layer0.propdown(layer0.top_down), layer0.unit)\n",
    "    \n",
    "    def free_energy_part(self, q, p):\n",
    "        return T.sum(T.add(q * (T.log(q + 1e-6) - T.log(p + 1e-6)),\n",
    "                           (1. - q) * (T.log(1. - q + 1e-6) - T.log(1. - p + 1e-6))))\n",
    "    \n",
    "    def free_energy(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        FEs = []\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # compute FE for each layer\n",
    "        FEs.append(self.free_energy_part(D, self.helmholtz_layers[0].prob_qp(awake)[1]))\n",
    "        for n in range(1, self.n_layers):\n",
    "            FEs.append(self.free_energy_part(self.helmholtz_layers[n-1].prob_qp(awake)[0],\n",
    "                                             self.helmholtz_layers[n].prob_qp(awake)[1]))\n",
    "        \n",
    "        # return function that can compute FE for given data\n",
    "        if return_fn:\n",
    "            return theano.function([D], T.sum(FEs), givens={self.v: D})\n",
    "        else:\n",
    "            return T.grad(-T.sum(FEs), params)\n",
    "       \n",
    "    def importance_weighting(self, log_q, log_p):\n",
    "        # from Bornschein et al., 2016\n",
    "        # w = sqrt(p/q)\n",
    "        log_w = (log_p - log_q) / 2.\n",
    "        # w_sum = sum_k(log_pq)\n",
    "        log_w_max = T.max(log_w, axis=1, keepdims=True)\n",
    "        log_w_sum = T.log(T.sum(T.exp(log_w - log_w_max), axis=1, keepdims=True)) + log_w_max\n",
    "        # w_norm = w/w_sum\n",
    "        log_w_norm = log_w - log_w_sum\n",
    "        # w = exp(log_w_norm)\n",
    "        return T.exp(log_w_norm)\n",
    "    \n",
    "    def log_likelihood(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        log_qs = []\n",
    "        log_ps = []\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # get log_q, log_p for each layer\n",
    "        for n in range(self.n_layers):\n",
    "            log_q_n, log_p_n = self.helmholtz_layers[n].log_prob(awake)\n",
    "            log_qs.append(log_q_n)\n",
    "            log_ps.append(log_p_n)\n",
    "            \n",
    "        # sum across layers\n",
    "        log_q = T.sum(log_qs, axis=0)\n",
    "        log_p = T.sum(log_ps, axis=0)\n",
    "        \n",
    "        # reshape to (batch_size, n_samples)\n",
    "        log_q = T.reshape(log_q, (D.shape[0]//self.n_samples, self.n_samples)) \n",
    "        log_p = T.reshape(log_p, (D.shape[0]//self.n_samples, self.n_samples))\n",
    "        \n",
    "        # get importance weights\n",
    "        w = self.importance_weighting(log_q, log_p)\n",
    "        \n",
    "        # compute log likelihood\n",
    "        log_pq = (log_p - log_q) / 2.\n",
    "        w_norm = T.log(w + 1e-6) - log_pq\n",
    "        LL = w_norm - T.log(self.n_samples)\n",
    "        \n",
    "        # cost\n",
    "        cost = T.sum(w * (log_p + log_q))\n",
    "            \n",
    "        # return theano function or LL\n",
    "        if return_fn:\n",
    "            return theano.function([D], -LL, givens={self.v: D})\n",
    "        else:\n",
    "            return T.grad(cost, params)\n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "    def train_function(self, train_data, batch_size, cost_type='free_energy', \n",
    "                       opts={'momentum': False, 'sparsity': False, 'sparsity_cost': 1., \n",
    "                             'sparsity_target': 0.001, 'n_samples': 1, 'auto_derivs': True}):\n",
    "        '''\n",
    "        Create training function with given update rule, momentum, and sparsity\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: theano matrix, training data to use\n",
    "        batch_size: int, size of each training mini-batch\n",
    "        cost_type: str, cost type to use for update rule ['free_energy' (default) or 'log_likelihood']\n",
    "        opts: dict, options for momentum, sparsity, importance sampling, and automatic differentiation\n",
    "            'momentum': bool, True/False use momentum [default: False]\n",
    "            'sparsity': bool, True/False use sparsity [default: False]\n",
    "            'sparsity_cost': float, scalar to multiply sparsity cost [default: 1.]\n",
    "            'sparsity_target': float, target sparsity for hidden activities [default: 0.001]\n",
    "            'n_samples': int, number of samples to use for importance sampling [default: 1] (cost_type='loglikelihood')\n",
    "            'auto_derivs': bool, True/False use automatic differentiation [default: True] (cost_type='free_energy')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_fn: theano function, training function with following inputs\n",
    "            idx: int, index of training mini-batch (i.e. train_data[idx*batch_size:(idx+1)*batch_size])\n",
    "            awake: bool, True/False awake for wake/sleep algorithm\n",
    "            m: float, momentum to apply (only if opts['momentum'] == True)\n",
    "            *lr: float(s), learning rates for each layer\n",
    "        '''\n",
    "        # init vars\n",
    "        awake = T.iscalar('awake').astype(theano.config.floatX)\n",
    "        lr = [T.scalar('lr_' + str(n)) / batch_size for n in range(self.n_layers)]\n",
    "        idx = T.iscalar('idx')\n",
    "        v = train_data[idx * batch_size:(idx + 1) * batch_size]\n",
    "        \n",
    "        # momentum options\n",
    "        if 'momentum' in opts:\n",
    "            momentum = opts['momentum']\n",
    "        else:\n",
    "            momentum = False\n",
    "        if momentum:\n",
    "            m = T.scalar('m')\n",
    "        else:\n",
    "            m = 0.\n",
    "        # sparsity options\n",
    "        if 'sparsity' in opts:\n",
    "            sparsity = opts['sparsity']\n",
    "        else:\n",
    "            sparsity = False\n",
    "        if 'spasity_cost' in opts:\n",
    "            sparsity_cost = opts['sparsity_cost']\n",
    "        elif sparsity:\n",
    "            sparsity_cost = 1.\n",
    "        if 'sparsity_target' in opts:\n",
    "            sparsity_target = opts['sparsity_target']\n",
    "        else:\n",
    "            sparsity_target = 0.001\n",
    "        # n_samples (add to self so log_likelihood() has access)\n",
    "        if 'n_samples' in opts and cost_type == 'log_likelihood':\n",
    "            self.n_samples = opts['n_samples']\n",
    "        else:\n",
    "            self.n_samples = 1\n",
    "        # automatic differentiation\n",
    "        if 'auto_derivs' in opts:\n",
    "            auto_derivs = opts['auto_derivs']\n",
    "        else:\n",
    "            auto_derivs = True\n",
    "            \n",
    "        # get gradient function\n",
    "        if cost_type == 'free_energy' or not auto_derivs:\n",
    "            print('Cost: free energy (automatic differentiation: %s)' % (auto_derivs))\n",
    "            grad_fn = self.free_energy\n",
    "        elif cost_type == 'log_likelihood' and auto_derivs:\n",
    "            print('Cost: log likelihood (n_samples: %d)' % (self.n_samples))\n",
    "            v = T.repeat(v, self.n_samples, axis=0)\n",
    "            grad_fn = self.log_likelihood\n",
    "        \n",
    "        # parameter updates for theano function\n",
    "        updates = []\n",
    "        for n in range(self.n_layers):\n",
    "            if auto_derivs: # automatic derivatives\n",
    "                wake_derivs = grad_fn(v, awake, self.helmholtz_layers[n].gen_params)\n",
    "                sleep_derivs = grad_fn(v, awake, self.helmholtz_layers[n].rec_params)\n",
    "            else: # manual derivatives\n",
    "                wake_derivs = self.helmholtz_layers[n].get_wake_derivs()\n",
    "                sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "                \n",
    "            # sparsity\n",
    "            if sparsity:\n",
    "                wake_sparse = self.helmholtz_layers[n].sparsity(sparsity_target, sparsity_cost,\n",
    "                                                                awake, self.helmholtz_layers[n].gen_params)\n",
    "                wake_derivs = [g + s for g, s in zip(wake_derivs, wake_sparse)]\n",
    "                sleep_sparse = self.helmholtz_layers[n].sparsity(sparsity_target, sparsity_cost,\n",
    "                                                                 awake, self.helmholtz_layers[n].rec_params)\n",
    "                sleep_derivs = [g + s for g, s in zip(sleep_derivs, sleep_sparse)]\n",
    "                \n",
    "            # wake phase\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].gen_params, wake_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[:2]):\n",
    "                updates.append((param, param + awake * lr[n] * gparam + inc_param * m))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, awake * (inc_param * m + lr[n] * gparam)))\n",
    "                \n",
    "            # sleep phase\n",
    "            sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].rec_params, sleep_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[2:]):\n",
    "                updates.append((param, param + (1. - awake) * lr[n] * gparam + inc_param * m))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, (1. - awake) * (inc_param * m + lr[n] * gparam)))\n",
    "        \n",
    "        # create train_fn\n",
    "        if momentum:\n",
    "            inputs = [idx, awake, m] + lr\n",
    "        else:\n",
    "            inputs = [idx, awake] + lr\n",
    "        train_fn = theano.function(inputs, [], updates=updates, givens={self.v: v})\n",
    "           \n",
    "        return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load_dataset('mnist.pkl.gz')[0][0]\n",
    "# data = T.gt(data, 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bar data\n",
    "data = np.zeros((1000, 3, 3), dtype=theano.config.floatX)\n",
    "for n in range(data.shape[0]):\n",
    "    # Horizontal bars 1/3, vertical bars 2/3\n",
    "    if np.random.rand() > (2./3.): \n",
    "        data[n, np.random.randint(data.shape[1]), :] = 1.\n",
    "    else:\n",
    "        data[n, :, np.random.randint(data.shape[2])] = 1.\n",
    "data = np.reshape(data, (data.shape[0], -1))\n",
    "data = T.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize helmholtz machine\n",
    "helm = HelmholtzMachine([784, 500, 10], unit='binary') #[9,6,1]\n",
    "# create training and cost monitoring functions\n",
    "batch_size = 10\n",
    "opts = {'momentum': True, 'sparsity': True, 'sparsity_cost': 0.5, 'sparsity_target': 0.001, 'n_samples': 10}\n",
    "train_fn = helm.train_function(data, batch_size, cost_type='free_energy', opts=opts)\n",
    "cost_fn = helm.log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "lr = [1e-3]*helm.n_layers\n",
    "n_batches = data.eval().shape[0]//batch_size\n",
    "monitor = n_batches//10 # monitor every 10%\n",
    "cost_history = []\n",
    "im_size = (28,28)\n",
    "\n",
    "# train in epochs\n",
    "for epoch in range(50):\n",
    "    # momentum\n",
    "    if epoch < 5:\n",
    "        m = 0.5\n",
    "    else:\n",
    "        m = 0.9\n",
    "    cost = []\n",
    "    # for each batch_size within n_batches, train\n",
    "    for n in range(n_batches):\n",
    "        train_fn(n, True, m, *lr) # wake phase\n",
    "        train_fn(n, False, m, *lr) # sleep phase\n",
    "        \n",
    "        # monitor samples\n",
    "        if n % monitor == 0:\n",
    "            # get cost for random set of mini-batches\n",
    "            rand_idxs = np.random.permutation(data.eval().shape[0])\n",
    "            cost.append(cost_fn(data[rand_idxs[:monitor*batch_size]].eval()))\n",
    "            \n",
    "            # print progress\n",
    "            clear_output(wait=True)\n",
    "            display('Epoch %d (%0.2f%%): %0.4f' % (epoch, 100. * n/n_batches, np.mean(cost)))\n",
    "            \n",
    "            # plot sample (sigmoid) from model\n",
    "            plt.imshow(helm.model_prob().eval().reshape(im_size), cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            # plot cost history\n",
    "            plt.plot(cost_history)\n",
    "            plt.show()\n",
    "            \n",
    "    # append cost history\n",
    "    cost_history.append(np.mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show samples (sigmoid) from model\n",
    "plt.imshow(helm.model_prob().eval().reshape(im_size), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
