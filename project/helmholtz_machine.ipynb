{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d, conv2d_transpose\n",
    "from theano.tensor.signal import pool\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "theano.config.compute_test_value = 'off'\n",
    "theano.config.floatX = 'float32'\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create shared variables for using gpu\n",
    "def shared_dataset(data, borrow=True, data_types=['float32','int32']):\n",
    "    if type(data) is not list:\n",
    "        data = list(data)\n",
    "    output = []\n",
    "    for i, x in enumerate(data):\n",
    "        output.append(theano.shared(np.asarray(x, dtype=data_types[i]), borrow=borrow))\n",
    "    return output\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    # get path/file for dataset\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the current directory.\n",
    "        new_path = os.path.join(os.curdir, dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "    # download from website\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "    # load from pickle\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # set test/valid/train sets\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    # combine datasets\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzLayer(object):\n",
    "    '''WRITEME'''\n",
    "    def __init__(self, input, n_in, n_out, unit='binary', th_rng=None, top_layer=False):\n",
    "        # init vars\n",
    "        self.input = input\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.unit = unit\n",
    "        # init th_rng if None\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(1)\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.top_layer = top_layer\n",
    "        \n",
    "        # recognition weights\n",
    "        self.WR = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_in,n_out)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WR')\n",
    "        # recognition biases\n",
    "        self.bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX), \n",
    "                                borrow=True, name='bR')\n",
    "        # generative weights\n",
    "        self.WG = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_out,n_in)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WG')\n",
    "        # generative biases\n",
    "        self.bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                borrow=True, name='bG')\n",
    "        \n",
    "        # if top_layer, remove shared WR, bR, WG\n",
    "        if self.top_layer:\n",
    "            self.WR = T.zeros_like(self.WR)\n",
    "            self.bR = T.zeros_like(self.bR)\n",
    "            self.WG = T.zeros_like(self.WG)\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.bG]\n",
    "            self.rec_params = []\n",
    "        else:\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.WG, self.bG]\n",
    "            self.rec_params = [self.WR, self.bR]\n",
    "        \n",
    "        # set output\n",
    "        self.output = self.sample_h_given_v(self.input)\n",
    "        self.q = self.prob(self.propup(theano.gradient.disconnected_grad(self.input)), self.unit)\n",
    "        self.p_0 = self.prob(self.propdown(theano.gradient.disconnected_grad(self.output)), self.unit)\n",
    "        \n",
    "        # init reconstr, top_down\n",
    "        self.reconstr = None\n",
    "        self.top_down = None\n",
    "        \n",
    "    def activation(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = T.nnet.sigmoid(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = u\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def sample(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = self.th_rng.binomial(size=u.shape, n=1, p=u, dtype=theano.config.floatX)\n",
    "        elif unit == 'gaussian':\n",
    "            y = T.add(u, self.th_rng.normal(u.shape, std=1., dtype=theano.config.floatX))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def prob(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            p = u\n",
    "        elif unit == 'gaussian':\n",
    "            p = (1./T.sqrt(2. * np.pi)) * T.exp(-T.sqr(u)/2.)\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return p\n",
    "    \n",
    "    def propup(self, v):\n",
    "        pre_act_h = T.dot(v, self.WR) + self.bR\n",
    "        return self.activation(pre_act_h, self.unit)\n",
    "    \n",
    "    def propdown(self, h):\n",
    "        pre_act_v = T.dot(h, self.WG) + self.bG\n",
    "        return self.activation(pre_act_v, self.unit)\n",
    "    \n",
    "    def sample_h_given_v(self, v):\n",
    "        h_mean = self.propup(v)\n",
    "        return self.sample(h_mean, self.unit)\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        v_mean = self.propdown(h)\n",
    "        return self.sample(v_mean, self.unit)\n",
    "    \n",
    "    def get_wake_derivs(self):\n",
    "        '''WRITEME'''\n",
    "        # get delta by propagating down with output\n",
    "        delta = self.propdown(self.output)\n",
    "        \n",
    "        # get wake derivatives\n",
    "        dWG = T.dot(self.output.T, (self.input - delta))\n",
    "        dbG = T.mean((self.input - delta), axis=0)\n",
    "        \n",
    "        # if top_layer, no WG derivs\n",
    "        if self.top_layer:\n",
    "            return [dbG]\n",
    "        else:\n",
    "            return dWG, dbG\n",
    "    \n",
    "    def get_sleep_derivs(self):\n",
    "        '''WRITEME'''\n",
    "        # if top_layer, no sleep derivs\n",
    "        if self.top_layer:\n",
    "            return []\n",
    "        \n",
    "        # get psi by propagating up with reconstr\n",
    "        psi = self.propup(self.reconstr)\n",
    "        \n",
    "        # get sleep derivatives\n",
    "        dWR = T.dot(self.reconstr.T, (self.top_down - psi))\n",
    "        dbR = T.mean((self.top_down - psi), axis=0)\n",
    "        return dWR, dbR\n",
    "        \n",
    "    def set_reconstr(self, top_down):\n",
    "        self.top_down = top_down\n",
    "        self.reconstr = self.sample_v_given_h(self.top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzMachine(object):\n",
    "    '''WRITEME'''\n",
    "    def __init__(self, n_ins, unit='binary', th_rng=None):\n",
    "        # init vars\n",
    "        self.n_layers = len(n_ins)\n",
    "        self.n_ins = n_ins\n",
    "        self.unit = unit\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(np.random.randint(2**30))\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "            \n",
    "        # init first layer input variable\n",
    "        self.v = T.matrix('v')\n",
    "        \n",
    "        # for each layer, append HelmholtzLayer\n",
    "        self.helmholtz_layers = []\n",
    "        for n in range(self.n_layers):\n",
    "            # set bG_1 to True if top layer, False otherwise\n",
    "            is_top_layer = (n == self.n_layers - 1)\n",
    "            # set input_layer\n",
    "            if n == 0:\n",
    "                input_layer = self.v\n",
    "            else:\n",
    "                input_layer = self.helmholtz_layers[-1].output\n",
    "            # set n_out\n",
    "            if is_top_layer:\n",
    "                n_out = 1\n",
    "            else:\n",
    "                n_out = self.n_ins[n+1]\n",
    "            # create helmholtz layer\n",
    "            self.helmholtz_layers.append(HelmholtzLayer(input_layer, \n",
    "                                                         self.n_ins[n], \n",
    "                                                         n_out,\n",
    "                                                         unit=self.unit,\n",
    "                                                         th_rng=self.th_rng,\n",
    "                                                         top_layer=is_top_layer))\n",
    "            \n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((1,1), dtype=theano.config.floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def model_sample(self):\n",
    "        return self.helmholtz_layers[0].reconstr[0]\n",
    "    \n",
    "    def free_energy_part(self, q, p):\n",
    "        return T.sum(T.add(q * (T.log(q + 1e-6) - T.log(p + 1e-6)),\n",
    "                           (1. - q) * (T.log(1. - q + 1e-6) - T.log(1. - p + 1e-6))))\n",
    "    \n",
    "    def free_energy(self, D=None):\n",
    "        FEs = []\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "        # compute FE for each layer\n",
    "        FEs.append(self.free_energy_part(D, self.helmholtz_layers[0].p_0))\n",
    "        for n in range(1, self.n_layers):\n",
    "            FEs.append(self.free_energy_part(self.helmholtz_layers[n-1].q, self.helmholtz_layers[n].p_0))\n",
    "        # return function that can compute FE for given data\n",
    "        if return_fn:\n",
    "            return theano.function([D], T.sum(FEs), givens={self.v: D})\n",
    "        else:\n",
    "            return T.sum(FEs)\n",
    "    \n",
    "    def train_function(self, train_data, batch_size, auto_derivs=False):\n",
    "        # init vars\n",
    "        awake = T.iscalar('awake').astype(theano.config.floatX)\n",
    "        lr = [T.scalar('lr_' + str(n)) / batch_size for n in range(self.n_layers)]\n",
    "        idx = T.iscalar('idx')\n",
    "        v = train_data[idx * batch_size:(idx + 1) * batch_size]  \n",
    "            \n",
    "        # parameter updates for theano function\n",
    "        updates = []\n",
    "        for n in range(self.n_layers):\n",
    "            if auto_derivs: # automatic derivatives\n",
    "                wake_derivs = T.grad(-self.free_energy(v), self.helmholtz_layers[n].gen_params)\n",
    "                sleep_derivs = T.grad(-self.free_energy(v), self.helmholtz_layers[n].rec_params)\n",
    "            else: # manual derivatives\n",
    "                wake_derivs = self.helmholtz_layers[n].get_wake_derivs()\n",
    "                sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "            # wake phase\n",
    "            for param, gparam in zip(self.helmholtz_layers[n].gen_params, wake_derivs):\n",
    "                updates.append((param, param + awake * lr[n] * gparam))\n",
    "            # sleep phase\n",
    "            sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "            for param, gparam in zip(self.helmholtz_layers[n].rec_params, sleep_derivs):\n",
    "                updates.append((param, param + (1. - awake) * lr[n] * gparam))\n",
    "        \n",
    "        # create train_fn\n",
    "        inputs = [idx, awake] + lr\n",
    "        train_fn = theano.function(inputs, [], updates=updates, givens={self.v: v})\n",
    "        \n",
    "        # set cost_fn\n",
    "        cost_fn = self.free_energy()\n",
    "           \n",
    "        return train_fn, cost_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = T.gt(load_dataset('mnist.pkl.gz')[0][0], 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bar data\n",
    "data = np.zeros((1000, 3, 3), dtype=theano.config.floatX)\n",
    "for n in range(data.shape[0]):\n",
    "    # Horizontal bars 1/3, vertical bars 2/3\n",
    "    if np.random.rand() > (2./3.): \n",
    "        data[n, np.random.randint(data.shape[1]), :] = 1.\n",
    "    else:\n",
    "        data[n, :, np.random.randint(data.shape[2])] = 1.\n",
    "data = np.reshape(data, (data.shape[0], -1))\n",
    "data = T.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize helmholtz machine\n",
    "helm = HelmholtzMachine([784, 300, 200, 100, 50, 25, 10], unit='binary') #[9,6,1]\n",
    "# create training function\n",
    "batch_size = 10\n",
    "train_fn, cost_fn = helm.train_function(data, batch_size, auto_derivs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "lr = [5e-3]*helm.n_layers\n",
    "n_batches = data.eval().shape[0]//batch_size\n",
    "monitor = n_batches//10 # monitor every 10%\n",
    "cost_history = []\n",
    "im_size = (28,28)\n",
    "\n",
    "# train in epochs\n",
    "for epoch in range(50):\n",
    "    cost = []\n",
    "    # for each batch_size within n_batches, train\n",
    "    for n in range(n_batches):\n",
    "        train_fn(n, True, *lr) # wake phase\n",
    "        train_fn(n, False, *lr) # sleep phase\n",
    "        \n",
    "        # monitor samples\n",
    "        if n % monitor == 0:\n",
    "            # get cost\n",
    "            rand_idxs = np.random.permutation(data.eval().shape[0])\n",
    "            cost.append(cost_fn(data[rand_idxs[:monitor*batch_size]].eval()))\n",
    "            # print progress\n",
    "            clear_output(wait=True)\n",
    "            display('Epoch %d (%0.2f%%): %0.4f' % (epoch, 100. * n/n_batches, np.mean(cost)))\n",
    "            # plot sample from model\n",
    "            plt.imshow(helm.model_sample().eval().reshape(im_size), cmap='gray')\n",
    "            plt.show()\n",
    "            # plot cost history\n",
    "            plt.plot(cost_history)\n",
    "            plt.show()\n",
    "    # append cost history\n",
    "    cost_history.append(np.mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(helm.model_sample().eval().reshape(im_size), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
