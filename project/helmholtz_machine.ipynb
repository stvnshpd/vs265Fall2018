{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d, conv2d_transpose\n",
    "from theano.tensor.signal import pool\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "theano.config.compute_test_value = 'off'\n",
    "theano.config.floatX = 'float32'\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create shared variables for using gpu\n",
    "def shared_dataset(data, borrow=True, data_types=['float32','int32']):\n",
    "    if type(data) is not list:\n",
    "        data = list(data)\n",
    "    output = []\n",
    "    for i, x in enumerate(data):\n",
    "        output.append(theano.shared(np.asarray(x, dtype=data_types[i]), borrow=borrow))\n",
    "    return output\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    # get path/file for dataset\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the current directory.\n",
    "        new_path = os.path.join(os.curdir, dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "    # download from website\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "    # load from pickle\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # set test/valid/train sets\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    # combine datasets\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzLayer(object):\n",
    "    '''\n",
    "    Helmholtz layer for Helmholtz Machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input: theano matrix, input to layer (data or output of previous layer)\n",
    "    n_in: int, number of input units\n",
    "    n_out: int, number of hidden units\n",
    "    unit: str, hidden unit type ['binary' (default) or 'gaussian']\n",
    "    sigma: float, sigma for gaussian units [default: 1.]\n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    top_layer: bool, True/False layer is top layer\n",
    "    k: float, fraction of most active units to keep; less active units are set to 0\n",
    "        [default: 0, no units set to 0]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzLayer\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Gaussian units assume 0 mean, 1 std (fixed sigma).\n",
    "    '''\n",
    "    def __init__(self, input, n_in, n_out, unit='binary', sigma=1., th_rng=None, top_layer=False, k=0):\n",
    "        # init vars\n",
    "        self.input = input\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.unit = unit\n",
    "        self.sigma = sigma\n",
    "        # init th_rng if None\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(1)\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.top_layer = top_layer\n",
    "        self.k = k\n",
    "        \n",
    "        # recognition weights\n",
    "        self.WR = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_in,n_out)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WR')\n",
    "        # recognition biases\n",
    "        self.bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX), \n",
    "                                borrow=True, name='bR')\n",
    "        # generative weights\n",
    "        self.WG = theano.shared(np.asarray(np.random.normal(scale=0.01, size=(n_out,n_in)),\n",
    "                                           dtype=theano.config.floatX),\n",
    "                                borrow=True, name='WG')\n",
    "        # generative biases\n",
    "        self.bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                borrow=True, name='bG')\n",
    "        \n",
    "        # momentum\n",
    "        self.inc_WR = theano.shared(np.zeros((n_in,n_out), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WR')\n",
    "        self.inc_bR = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bR')\n",
    "        self.inc_WG = theano.shared(np.zeros((n_out,n_in), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_WG') \n",
    "        self.inc_bG = theano.shared(np.zeros((n_in,), dtype=theano.config.floatX),\n",
    "                                    borrow=True, name='inc_bG')\n",
    "        \n",
    "        # if top_layer, remove shared WR, bR, WG\n",
    "        if self.top_layer:\n",
    "            self.WR = T.zeros_like(self.WR)\n",
    "            self.bR = T.zeros_like(self.bR)\n",
    "            self.WG = T.zeros_like(self.WG)\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.bG]\n",
    "            self.rec_params = []\n",
    "            self.inc_params = [self.inc_bG]\n",
    "        else:\n",
    "            # set gen_params, rec_params\n",
    "            self.gen_params = [self.WG, self.bG]\n",
    "            self.rec_params = [self.WR, self.bR]\n",
    "            self.inc_params = [self.inc_WG, self.inc_bG, self.inc_WR, self.inc_bR]\n",
    "        \n",
    "        # set output\n",
    "        self.output = self.sample_h_given_v(self.input)\n",
    "        \n",
    "        # init reconstr, top_down\n",
    "        self.reconstr = None\n",
    "        self.top_down = None\n",
    "        \n",
    "    def activation(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            u *= (1./T.square(self.sigma))\n",
    "            y = T.nnet.sigmoid(u)\n",
    "        elif unit == 'gaussian':\n",
    "            y = u\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def sample(self, u, unit):\n",
    "        if unit == 'binary':\n",
    "            y = self.th_rng.binomial(size=u.shape, n=1, p=u, dtype=theano.config.floatX)\n",
    "        elif unit == 'gaussian':\n",
    "            y = T.add(u, self.th_rng.normal(u.shape, std=self.sigma, dtype=theano.config.floatX))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return y\n",
    "    \n",
    "    def prob(self, u, x, unit):\n",
    "        if unit == 'binary':\n",
    "            p = T.pow(u, x) * T.pow(1. - u, 1. - x)\n",
    "        elif unit == 'gaussian':\n",
    "            p = (1./T.sqrt(2. * np.pi * (self.sigma**2))) * T.exp(-T.sqr(x - u) / (2. * (self.sigma**2)))\n",
    "        else: # throw error\n",
    "            raise NotImplementedError\n",
    "        return p\n",
    "    \n",
    "    def propup(self, v):\n",
    "        pre_act_h = T.dot(v, self.WR) + self.bR\n",
    "        # if k-sparse, apply\n",
    "        pre_act_h = self.k_sparse(pre_act_h, self.k)\n",
    "        return self.activation(pre_act_h, 'binary')\n",
    "    \n",
    "    def propdown(self, h):\n",
    "        pre_act_v = T.dot(h, self.WG) + self.bG\n",
    "        # if k-sparse, apply\n",
    "        pre_act_v = self.k_sparse(pre_act_v, self.k)\n",
    "        return self.activation(pre_act_v, self.unit)\n",
    "    \n",
    "    def sample_h_given_v(self, v):\n",
    "        h_mean = self.propup(v)\n",
    "        return self.sample(h_mean, 'binary')\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        v_mean = self.propdown(h)\n",
    "        return self.sample(v_mean, self.unit)\n",
    "    \n",
    "    def get_wake_derivs(self):\n",
    "        # get delta by propagating down with output\n",
    "        delta = self.propdown(self.output)\n",
    "        \n",
    "        # get wake derivatives\n",
    "        dWG = T.dot(self.output.T, (self.input - delta))/T.cast(self.input.shape[0], dtype=theano.config.floatX)\n",
    "        dbG = T.mean((self.input - delta), axis=0)\n",
    "        \n",
    "        # if top_layer, no WG derivs\n",
    "        if self.top_layer:\n",
    "            return [dbG]\n",
    "        else:\n",
    "            return [dWG, dbG]\n",
    "        \n",
    "    def get_sleep_derivs(self):\n",
    "        # if top_layer, no sleep derivs\n",
    "        if self.top_layer:\n",
    "            return []\n",
    "        \n",
    "        # get psi by propagating up with reconstr\n",
    "        psi = self.propup(self.reconstr)\n",
    "        \n",
    "        # get sleep derivatives\n",
    "        dWR = T.dot(self.reconstr.T, (self.top_down - psi))/T.cast(self.reconstr.shape[0], dtype=theano.config.floatX)\n",
    "        dbR = T.mean((self.top_down - psi), axis=0)\n",
    "        return [dWR, dbR]\n",
    "    \n",
    "    def switch_awake(self, awake):\n",
    "        # set x,y based on wake or sleep\n",
    "        x = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.input) + awake,\n",
    "                                                       self.input, self.reconstr))\n",
    "        y = theano.gradient.disconnected_grad(T.switch(T.zeros_like(self.output) + awake,\n",
    "                                                       self.output, self.top_down))\n",
    "        return x, y\n",
    "    \n",
    "    def log_prob(self, awake):\n",
    "        # get x, y\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # compute log probs\n",
    "        log_q = T.sum(T.log(self.prob(self.propup(x), y, 'binary') + 1e-6), axis=1)\n",
    "        log_p = T.sum(T.log(self.prob(self.propdown(y), x, self.unit) + 1e-6), axis=1)\n",
    "        return log_q, log_p\n",
    "    \n",
    "    def MSE_loss(self, awake):\n",
    "        # set x,y based on wake or sleep\n",
    "        x, y = self.switch_awake(awake)\n",
    "        # return MSE loss\n",
    "        return T.switch(awake, \n",
    "                        T.mean(T.square(x - self.propdown(y))), \n",
    "                        T.mean(T.square(y - self.propup(x))))\n",
    "        \n",
    "    \n",
    "    def k_sparse(self, x, k):\n",
    "        # get threshold and repeat across axis=1\n",
    "        k = T.switch(T.gt(k, x.shape[1]), 0, k)\n",
    "        thr = T.sort(x)[:, -k].dimshuffle((0,'x'))\n",
    "        thr = T.repeat(thr, x.shape[1], axis=1)\n",
    "        # set values >= thr to x, values <= thr to 0\n",
    "        return T.switch(T.ge(T.abs_(x), thr), x, T.zeros_like(x))\n",
    "    \n",
    "    def set_reconstr(self, top_down):\n",
    "        self.top_down = top_down\n",
    "        self.reconstr = self.sample_v_given_h(self.top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HelmholtzMachine(object):\n",
    "    '''\n",
    "    Helmholtz machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_ins: list of ints, number of inputs for each layer\n",
    "    vis_unit: str, visible unit type ['binary' (default) or 'gaussian']\n",
    "    sigma: float, sigma for gaussian visible units [default: 1.] \n",
    "    th_rng: theano RandomStreams, random generator for sampling\n",
    "    k: float, fraction of most active units to keep (less active units set to 0) [default: 0]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    HelmholtzMachine\n",
    "    '''\n",
    "    def __init__(self, n_ins, vis_unit='binary', sigma=1., th_rng=None, k=0):\n",
    "        # init vars\n",
    "        self.n_layers = len(n_ins)\n",
    "        self.n_ins = n_ins\n",
    "        self.vis_unit = vis_unit\n",
    "        self.sigma = sigma\n",
    "        if th_rng is None:\n",
    "            self.th_rng = RandomStreams(np.random.randint(2**30))\n",
    "        else:\n",
    "            self.th_rng = th_rng\n",
    "        self.k = k\n",
    "            \n",
    "        # init first layer input variable\n",
    "        self.v = T.matrix('v')\n",
    "        \n",
    "        # for each layer, append HelmholtzLayer\n",
    "        self.helmholtz_layers = []\n",
    "        for n in range(self.n_layers):\n",
    "            # set bG_1 to True if top layer, False otherwise\n",
    "            is_top_layer = (n == self.n_layers - 1)\n",
    "            # set input_layer\n",
    "            if n == 0:\n",
    "                input_layer = self.v\n",
    "            else:\n",
    "                input_layer = self.helmholtz_layers[-1].output\n",
    "            # set n_out\n",
    "            if is_top_layer:\n",
    "                n_out = 1\n",
    "            else:\n",
    "                n_out = self.n_ins[n+1]\n",
    "            # set vis unit type\n",
    "            if n == 0:\n",
    "                unit = self.vis_unit\n",
    "                sigma = self.sigma\n",
    "            else: # set binary for all other hidden units\n",
    "                unit = 'binary'\n",
    "                sigma = 1.\n",
    "            # create helmholtz layer\n",
    "            self.helmholtz_layers.append(HelmholtzLayer(input_layer, \n",
    "                                                         self.n_ins[n], \n",
    "                                                         n_out,\n",
    "                                                         unit=unit,\n",
    "                                                         sigma=sigma,\n",
    "                                                         th_rng=self.th_rng,\n",
    "                                                         top_layer=is_top_layer,\n",
    "                                                         k=self.k))\n",
    "            \n",
    "        # for each layer, set reconstr\n",
    "        for n in range(self.n_layers-1, -1, -1):\n",
    "            # for top layer, top_down is zeros\n",
    "            if n == self.n_layers-1:\n",
    "                top_down = T.zeros((self.n_ins[-1],1), dtype=theano.config.floatX)\n",
    "            else:\n",
    "                top_down = self.helmholtz_layers[n+1].reconstr\n",
    "            # set top_down and reconstr\n",
    "            self.helmholtz_layers[n].set_reconstr(top_down)\n",
    "    \n",
    "    def model_sample(self):\n",
    "        return self.helmholtz_layers[0].reconstr[0][0]\n",
    "    \n",
    "    def model_prob(self):\n",
    "        layer0 = self.helmholtz_layers[0]\n",
    "        return layer0.propdown(layer0.top_down)[0]\n",
    "    \n",
    "    def sample_fn(self, sample_type='sample'):\n",
    "        if type(self.k) is T.TensorVariable:\n",
    "            givens = {self.k: T.cast(0, dtype='int32')}\n",
    "        else:\n",
    "            givens = {}\n",
    "        if sample_type == 'sample':\n",
    "            return theano.function([], self.model_sample(), givens=givens)\n",
    "        else:\n",
    "            return theano.function([], self.model_prob(), givens=givens)\n",
    "        \n",
    "    def MSE(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        # init D\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "        \n",
    "        # compute error for each layer\n",
    "        error = 0.\n",
    "        for n in range(self.n_layers):\n",
    "            error += self.helmholtz_layers[n].MSE_loss(awake)\n",
    "        \n",
    "        # return theano function or grad\n",
    "        if return_fn:\n",
    "            if type(self.k) is T.TensorVariable:\n",
    "                givens = {self.v: D, self.k: T.cast(0, dtype='int32')}\n",
    "            else:\n",
    "                givens = {self.v: D}\n",
    "            return theano.function([D], error, givens=givens)\n",
    "        else:\n",
    "            return T.grad(error, params)\n",
    "            \n",
    "    \n",
    "    def free_energy(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        # init D\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # compute FE for each layer (log_q of the data is 0)\n",
    "        FE = 0.\n",
    "        FE -= self.helmholtz_layers[0].log_prob(awake)[1]\n",
    "        for n in range(1, self.n_layers):\n",
    "            log_q = self.helmholtz_layers[n-1].log_prob(awake)[0]\n",
    "            log_p = self.helmholtz_layers[n].log_prob(awake)[1]\n",
    "            FE += (log_q - log_p)\n",
    "        \n",
    "        # return theano function or grad\n",
    "        if return_fn:\n",
    "            if type(self.k) is T.TensorVariable:\n",
    "                givens = {self.v: D, self.k: T.cast(0, dtype='int32')}\n",
    "            else:\n",
    "                givens = {self.v: D}\n",
    "            return theano.function([D], -T.sum(FE), givens=givens)\n",
    "        else:\n",
    "            return T.grad(-T.sum(FE), params)\n",
    "       \n",
    "    def importance_weighting(self, log_q, log_p):\n",
    "        # from Bornschein et al., 2016\n",
    "        # w = sqrt(p/q)\n",
    "        log_w = (log_p - log_q) / 2.\n",
    "        # w_sum = sum_k(log_pq)\n",
    "        log_w_max = T.max(log_w, axis=1, keepdims=True)\n",
    "        log_w_sum = T.log(T.sum(T.exp(log_w - log_w_max), axis=1, keepdims=True)) + log_w_max\n",
    "        # w_norm = w/w_sum\n",
    "        log_w_norm = log_w - log_w_sum\n",
    "        # w = exp(log_w_norm)\n",
    "        return T.exp(log_w_norm)\n",
    "    \n",
    "    def log_likelihood(self, D=None, awake=T.cast(1., 'int32'), params=[]):\n",
    "        # init D\n",
    "        if D is None:\n",
    "            return_fn = True\n",
    "            D = T.matrix('D')\n",
    "        else:\n",
    "            return_fn = False\n",
    "            \n",
    "        # get log_q, log_p for each layer\n",
    "        log_qs = []\n",
    "        log_ps = []\n",
    "        for n in range(self.n_layers):\n",
    "            log_q_n, log_p_n = self.helmholtz_layers[n].log_prob(awake)\n",
    "            log_qs.append(log_q_n)\n",
    "            log_ps.append(log_p_n)\n",
    "            \n",
    "        # sum across layers\n",
    "        log_q = T.sum(log_qs, axis=0)\n",
    "        log_p = T.sum(log_ps, axis=0)\n",
    "        \n",
    "        # reshape to (batch_size, n_samples)\n",
    "        log_q = T.reshape(log_q, (D.shape[0]//self.n_samples, self.n_samples)) \n",
    "        log_p = T.reshape(log_p, (D.shape[0]//self.n_samples, self.n_samples))\n",
    "        \n",
    "        # get importance weights\n",
    "        w = self.importance_weighting(log_q, log_p)\n",
    "        \n",
    "        # compute log likelihood\n",
    "        log_pq = (log_p - log_q) / 2.\n",
    "        log_w_sum = log_pq - T.log(w + 1e-6)\n",
    "        LL = log_w_sum - T.log(self.n_samples)\n",
    "        \n",
    "        # cost\n",
    "        cost = T.sum(w * (log_p + log_q))\n",
    "            \n",
    "        # return theano function or grad\n",
    "        if return_fn:\n",
    "            if type(self.k) is T.TensorVariable:\n",
    "                givens = {self.v: D, self.k: T.cast(0, dtype='int32')}\n",
    "            else:\n",
    "                givens = {self.v: D}\n",
    "            return theano.function([D], LL, givens=givens)\n",
    "        else:\n",
    "            return T.grad(cost, params)\n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "    def show_W(self, layer_idx, W_idx=0, W_type='generative'):\n",
    "        # get WG\n",
    "        if W_type == 'generative':\n",
    "            W = self.helmholtz_layers[layer_idx].WG.T.eval()\n",
    "        else: # get WR\n",
    "            W = self.helmholtz_layers[layer_idx].WR.eval()\n",
    "        # get img shape\n",
    "        img_shape = [int(np.sqrt(W.shape[0]))]*2\n",
    "        # show W\n",
    "        plt.imshow(W[:,W_idx].reshape(img_shape), cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    def train_function(self, train_data, batch_size, cost_type='free_energy', \n",
    "                       opts={'momentum': False, 'k_sparse': False, 'n_samples': 1, 'auto_derivs': True}):\n",
    "        '''\n",
    "        Create training function with given update rule, momentum, and sparsity\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: theano matrix, training data to use\n",
    "        batch_size: int, size of each training mini-batch\n",
    "        cost_type: str, cost type to use for update rule ['free_energy' (default) or 'log_likelihood']\n",
    "        opts: dict, options for momentum, sparsity, importance sampling, and automatic differentiation\n",
    "            'momentum': bool, True/False use momentum [default: False]\n",
    "            'k_sparse': bool, True/False to do k-sparse (includes new input for train_fn) [default: False]\n",
    "            'n_samples': int, number of samples to use for importance sampling [default: 1] (cost_type='loglikelihood')\n",
    "            'auto_derivs': bool, True/False use automatic differentiation [default: True] (cost_type='free_energy')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_fn: theano function, training function with following inputs\n",
    "            idx: int, index of training mini-batch (i.e. train_data[idx*batch_size:(idx+1)*batch_size])\n",
    "            awake: bool, True/False awake for wake/sleep algorithm\n",
    "            m: float, momentum to apply (if opts['momentum'] == True)\n",
    "            k: float, fraction of most active units to keep (less active set to 0; if opts['k_sparse'] == True)\n",
    "            *lr: float(s), learning rates for each layer\n",
    "            \n",
    "        Note\n",
    "        ----\n",
    "        In order to use k-sparse, when initializing HelmholtzMachine set k=T.scalar('k').\n",
    "        '''\n",
    "        # init vars\n",
    "        awake = T.scalar('awake')\n",
    "        lr = T.scalar('lr')\n",
    "        idx = T.iscalar('idx')\n",
    "        v = train_data[idx * batch_size:(idx + 1) * batch_size]\n",
    "        \n",
    "        # momentum options\n",
    "        if 'momentum' in opts:\n",
    "            momentum = opts['momentum']\n",
    "        else:\n",
    "            momentum = False\n",
    "        if momentum:\n",
    "            m = T.scalar('m')\n",
    "        else:\n",
    "            m = 0.\n",
    "        # sparsity options\n",
    "        if 'k_sparse' in opts and opts['k_sparse']:\n",
    "            k_sparse = opts['k_sparse']\n",
    "            k = T.iscalar('k')\n",
    "        else:\n",
    "            k_sparse = False\n",
    "            k = T.cast(0, dtype='int32')\n",
    "        # n_samples (add to self so log_likelihood() has access)\n",
    "        if 'n_samples' in opts and cost_type == 'log_likelihood':\n",
    "            self.n_samples = opts['n_samples']\n",
    "        else:\n",
    "            self.n_samples = 1\n",
    "        # automatic differentiation\n",
    "        if 'auto_derivs' in opts:\n",
    "            auto_derivs = opts['auto_derivs']\n",
    "        else:\n",
    "            auto_derivs = True\n",
    "            \n",
    "        # get gradient function\n",
    "        if cost_type == 'free_energy':\n",
    "            print('Cost: free energy (automatic differentiation: %s)' % (auto_derivs))\n",
    "            grad_fn = self.free_energy\n",
    "        elif cost_type == 'log_likelihood':\n",
    "            print('Cost: log likelihood (n_samples: %d)' % (self.n_samples))\n",
    "            v = T.repeat(v, self.n_samples, axis=0)\n",
    "            grad_fn = self.log_likelihood\n",
    "            auto_derivs = True\n",
    "        \n",
    "        # parameter updates for theano function\n",
    "        updates = []\n",
    "        for n in range(self.n_layers):\n",
    "            if auto_derivs: # automatic derivatives\n",
    "                wake_derivs = grad_fn(v, 1, self.helmholtz_layers[n].gen_params)\n",
    "                sleep_derivs = grad_fn(v, 1, self.helmholtz_layers[n].rec_params)\n",
    "            else: # manual derivatives\n",
    "                wake_derivs = self.helmholtz_layers[n].get_wake_derivs()\n",
    "                sleep_derivs = self.helmholtz_layers[n].get_sleep_derivs()\n",
    "                \n",
    "            # wake phase\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].gen_params, wake_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[:2]):\n",
    "                # set awake 1 if auto_derivs\n",
    "                if auto_derivs:\n",
    "                    awake = 1.\n",
    "                # update params\n",
    "                updates.append((param, param + awake * (inc_param * m + lr * gparam)))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, awake * (inc_param * m + lr * gparam)))\n",
    "                \n",
    "            # sleep phase\n",
    "            for param, gparam, inc_param in zip(self.helmholtz_layers[n].rec_params, sleep_derivs,\n",
    "                                                self.helmholtz_layers[n].inc_params[2:]):\n",
    "                # set awake 0 if auto_derivs\n",
    "                if auto_derivs:\n",
    "                    awake = 0.\n",
    "                # update params\n",
    "                updates.append((param, param + (1. - awake) * (lr * gparam + inc_param * m)))\n",
    "                # momentum\n",
    "                if momentum:\n",
    "                    updates.append((inc_param, (1. - awake) * (inc_param * m - lr * gparam)))\n",
    "        \n",
    "        # create train_fn\n",
    "        inputs = [idx]\n",
    "        if not auto_derivs:\n",
    "            inputs += [awake]\n",
    "        if momentum:\n",
    "            inputs += [m]\n",
    "        if k_sparse:\n",
    "            inputs += [k]\n",
    "            givens = {self.v: v, self.k: k}\n",
    "        else:\n",
    "            givens = {self.v: v}\n",
    "        inputs += [lr]\n",
    "        print('Inputs:', inputs)\n",
    "        train_fn = theano.function(inputs, [], updates=updates, givens=givens)\n",
    "           \n",
    "        return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MNIST\n",
    "data = load_dataset('data/mnist.pkl.gz')[0][0]\n",
    "data = T.gt(data, 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bar data\n",
    "data = np.zeros((1000, 3, 3), dtype=theano.config.floatX)\n",
    "for n in range(data.shape[0]):\n",
    "    # Horizontal bars 1/3, vertical bars 2/3\n",
    "    if np.random.rand() > (2./3.): \n",
    "        data[n, np.random.randint(data.shape[1]), :] = 1.\n",
    "    else:\n",
    "        data[n, :, np.random.randint(data.shape[2])] = 1.\n",
    "data = np.reshape(data, (data.shape[0], -1))\n",
    "data = T.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# natural image patches\n",
    "from scipy.io import loadmat\n",
    "# load pre-whitened natural images\n",
    "data0 = loadmat('data/olshausen_single.mat')['images_all']\n",
    "# create dataset of cropped images\n",
    "data = []\n",
    "crop = (28,28)\n",
    "data_size = 10000\n",
    "for i in range(data_size):\n",
    "    n = np.random.randint(data0.shape[1])\n",
    "    x_idx = np.random.randint(data0[0, n].shape[0]-crop[0])\n",
    "    y_idx = np.random.randint(data0[0, n].shape[1]-crop[1])\n",
    "    data.append(data0[0, n][x_idx:x_idx+crop[0], y_idx:y_idx+crop[1]])\n",
    "    # demean\n",
    "    data[-1] -= np.mean(data[-1])\n",
    "data = np.asarray(data, dtype=theano.config.floatX)\n",
    "# reshape data\n",
    "data = np.reshape(data, (data_size, -1))\n",
    "# make data tensor \n",
    "data = T.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize helmholtz machine\n",
    "helm = HelmholtzMachine([28*28, 128, 32, 16], vis_unit='binary', sigma=1., k=T.iscalar('k'))\n",
    "# create training and cost monitoring functions\n",
    "batch_size = 16\n",
    "opts = {'momentum': True, 'k_sparse': True, 'auto_derivs': False, 'n_samples': 10}\n",
    "train_fn = helm.train_function(data, batch_size, cost_type='free_energy', opts=opts)\n",
    "cost_fn = helm.MSE()\n",
    "model_prob = helm.sample_fn(sample_type='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "lr = 1e-2\n",
    "n_batches = data.eval().shape[0]//batch_size\n",
    "monitor = n_batches//20 # monitor every 10%\n",
    "cost_history = []\n",
    "im_size = (28,28)\n",
    "k = 50\n",
    "\n",
    "# train in epochs\n",
    "for epoch in range(0, 50):\n",
    "    # momentum\n",
    "    if epoch < 5:\n",
    "        m = 0.9\n",
    "    else:\n",
    "        m = 0.9\n",
    "    k = np.maximum(k - 1, 25)\n",
    "    cost = []\n",
    "    # for each batch_size within n_batches, train\n",
    "    for n in range(n_batches):\n",
    "        train_fn(n, True, m, k, lr) # wake phase\n",
    "        train_fn(n, False, m, k, lr) # sleep phase\n",
    "        \n",
    "        # monitor samples\n",
    "        if n % monitor == 0:\n",
    "            # get cost for random set of mini-batches\n",
    "            rand_idxs = np.random.permutation(data.eval().shape[0])\n",
    "            cost.append(cost_fn(data[rand_idxs[:batch_size]].eval()))\n",
    "            \n",
    "            # print progress\n",
    "            clear_output(wait=True)\n",
    "            display('Epoch %d (%0.2f%%): %0.4f' % (epoch, 100. * n/n_batches, np.mean(cost)))\n",
    "            \n",
    "            # plot sample (sigmoid) from model\n",
    "            plt.imshow(model_prob().reshape(im_size), cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            # show weight\n",
    "            helm.show_W(0, 0)\n",
    "            \n",
    "            # plot cost history\n",
    "            plt.plot(cost_history)\n",
    "            plt.show()\n",
    "            \n",
    "    # append cost history\n",
    "    cost_history.append(np.mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n +=1\n",
    "helm.show_W(0, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
